
## I. Named Entity Recognition (NER) Task

|Concept|Description|
|:--|:--|
|**Named Entity**|An entity represented by a name, such as a person, organization, location, number, or product.|
|**NER Definition**|The task of finding and classifying these named entities within text.|
|**Utility**|Fundamental to NLP: used for building knowledge bases (information extraction), answering questions, and requiring special care in machine translation (e.g., transliteration or number conversion).|
|**Challenges**|High cost of obtaining labeled data, need for comprehensive world knowledge (e.g., identifying "Sheffield Wednesday F.C." as an organization), and handling ambiguity/long context dependencies.|



## II. Multiword Labels and Evaluation

NER is typically modeled as a **Sequence Labeling** task, assigning one label per word.

### 2.1 Span Representation

This addresses how to label entities composed of multiple words:

|Scheme|Description|Labels|Notes|
|:--|:--|:--|:--|
|**IO**|Inside (NE Type) or Outside (`O`).|#NE + 1|Cannot distinguish adjacent spans of the same entity type (e.g., two cities side-by-side).|
|**BIO**|Begin (`B-Type`), Inside (`I-Type`), or Outside (`O`).|2 $\times$ #NE + 1|Commonly used, addresses the boundary ambiguity of IO.|
|**BILOU**|Begin, Inside, Last (`L-Type`), Outside, or Unique (`U-Type`).|4 $\times$ #NE + 1|Provides the most specific boundary markers.|

### 2.2 Evaluation

Standard information retrieval metrics are used:

- **Metrics:** Precision, Recall, and F1 Score (harmonic mean of Precision and Recall).
- **Phrase-level Counting:** A prediction is only correct if the **entire span (boundary)** and its **type** are matched exactly. This method penalizes partial overlaps.
- **Token-level Counting:** Counts the number of correctly labeled individual words (tokens). This method rewards partial overlaps, but tends to weigh longer entities more heavily.



## III. Basic Neural Sequence Labeling Models

Early methods included Hidden Markov Models (HMM) and Conditional Random Fields (CRF). Today, the focus is on Neural Sequence Models.

### 3.1 Basic Components

A model typically includes 1-hot encoding, word embedding, a sequence layer (the core mechanism), and a classification layer. Training relies on the Cross-Entropy loss function.

### 3.2 Context Modeling Architectures

|Architecture|Context Focus|Characteristics|Limitations|
|:--|:--|:--|:--|
|**Windowing**|**Local Context**|Uses a fixed window size ($m+n+1$ words) around the current word. Often implemented using CNN or TDNN. Predictions are independent.|Cannot access long-distance dependencies effectively.|
|**RNN**|**Global Context**|Stores sequence history in a single, fixed-size hidden state. Trained via Backpropagation Through Time.|Fixed-size vector struggles to store long-range dependencies. Calculation is sequential and dependent on sequence length, preventing parallelism.|



## IV. Core Mechanism: Self-Attention

Self-attention was developed to overcome the RNN's limitations regarding fixed hidden state size and sequential computation.

### 4.1 Attention Model (QKV)

> Q: 在自注意力机制中，查询（Query）、键（Key）和值（Value）这三个向量是如何协同工作的？
> A: 查询向量与所有键向量计算相似度，该相似度作为权重来对相应的值向量进行加权求和。这个过程准确地描述了缩放点积注意力的核心：查询当前位置需要关注哪些其他位置（通过与键的相似度），然后汇集这些位置的信息（通过对值的加权求和）。
> 

The mechanism is inspired by database retrieval:

- **Query ($\mathbf{Q}$):** Used to query the other states in the sequence.
- **Key ($\mathbf{K}$):** Used for comparison to determine relevance.
- **Value ($\mathbf{V}$):** The components that are summed up to form the output.
- **Calculation:** The attention score (energy function) is typically calculated using the **Scaled Dot-Product**. The result is normalized using Softmax to create a probability distribution, which prevents gradient vanishing/exploding. The final output is the weighted sum of the values.

### 4.2 Multi-head Attention

This technique allows the model to focus on different aspects of the input simultaneously. It splits the hidden state into several "heads," calculating attention independently for each sub-vector, and concatenating the results. The total number of model parameters remains unchanged.

### 4.3 Positional Information

Self-attention treats the input as a Bag-of-Words, losing vital sequence order. Therefore, **Positional Encoding** is required.

- **Trigonometric Functions:** Adds sine and cosine functions to the word embeddings based on position. This periodic feature allows the model to extrapolate to unseen sequence lengths.
- **Rotary Positional Encodings (ROPE):** Rotates the Query and Key vectors based on absolute position. The goal is to ensure the final dot product depends only on the relative positions between words. This is applied in each layer.



## V. Advanced Architectures and Comparison

### 5.1 Advanced Techniques

1. **Multi-layer:** Stacking sequence layers increases the model's capacity and depth.
2. **Residual Connection:** Adds the input vector ($x$) to the output of a sub-layer ($F(x)$) ($h = F(x) + x$). This stabilizes the gradient flow, enabling the training of very deep networks.

### 5.2 Directionality and Receptive Field

- **Direction:** RNNs are inherently unidirectional (left-to-right). **Bidirectional RNNs (BiRNNs)** combine a forward and a backward RNN to capture context from both directions. Self-attention and TDNN/CNN have no inherent direction.
- **Receptive Field (Context Access):** TDNN/CNN only access local context; RNNs access the previous sequence; and self-attention accesses the entire input sequence at once.

### 5.3 Model Types and Consistency

- **Non-Autoregressive Models (e.g., standard RNN output):** Output predictions are independent of previous label predictions. They suffer from problems like inconsistent or repeated labels.
- **Autoregressive Models:** Current label prediction relies on the label predicted in the previous step.
- **RNN + CRF:** To ensure label consistency, a CRF (Conditional Random Field) layer is often placed on top of a neural model. The CRF learns the legal transitions between labels (e.g., an "I-City" cannot follow a "B-Person"), enforcing global output consistency.
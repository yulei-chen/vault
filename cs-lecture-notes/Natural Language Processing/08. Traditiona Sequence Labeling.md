
##  **I. Core Concept: What is Sequence Labeling?**

**1. Motivation:** Language, whether in the form of words, sentences, or documents, is often an input sequence with a variable length. The properties of each element within that sequence depend heavily on its context. Therefore, to accurately analyze language, we must use techniques that allow for **jointly modeling sequences**.

**2. Sequence Labeling Definition:** Sequence labeling is an NLP task where the input is a variable-length sequence, and the output is a label sequence of the same length. Roughly, the goal is to **predict a label for every item in the input sequence**.

**3. Applications:** Sequence labeling is crucial in NLP for tasks such as **Part-of-Speech (POS) Tagging** and Named Entity Recognition. It is also applied in other areas like computational biology, speech processing, and video processing.

##  **II. Core Application: Part-of-Speech Tagging (POS)**

**1. Task Purpose:** POS Tagging is a form of grammatical tagging focused on **word-category disambiguation**. The task requires marking a word in a text according to a specific part of speech (e.g., noun, verb) based on its definition and its **context**.

**2. The Ambiguity Challenge:** The primary difficulty in POS tagging is ambiguity, as words often have **more than one possible POS tag**.

- In one corpus (WSJ), 55% of tokens (word instances) were found to be ambiguous (having 2 or more tags).
- _Solution:_ Context is essential for disambiguation, necessitating the use of neighboring words and **neighboring POS tags**. This confirms the need for models capable of joint modeling of the full sentence.

##  **III. Sequence Labeling Models Overview**

Directly modeling the probability $P(y|x)$ or $P(x, y)$ is intractable because the number of possible tag sequences is **exponential** (calculated as ${\text{#Classes}}^{\text{#Words}}$). To simplify the modeling, we rely on sequence dependence.

The two main traditional models discussed are:

|        Model Type        |                   Model Name                    |         Models/Calculates         |                     Key Feature                     |
| :----------------------: | :---------------------------------------------: | :-------------------------------: | :-------------------------------------------------: |
|   **Generative Model**   |          **Hidden Markov Model (HMM)**          |    Joint probability $P(x, y)$    | Relies on strict independence assumptions (Markov). |
| **Discriminative Model** | **Linear Chain Conditional Random Field (CRF)** | Conditional probability $P(y\|x)$ |                                                     |

##  **IV. Model I: Hidden Markov Model (HMM)**

HMM is a **Generative Model** that assigns a joint probability to paired observation sequences ($O$) and label sequences ($Q$).

**1. Core Assumptions:**

- **Markov Assumption:** The probability of the current state ($q_t$) depends **only on the previous state** ($q_{t-1}$).
- **Independence of Outputs:** The probability of an observation (word) only depends on the current state (tag).

**2. HMM Elements (Parameters $\lambda$):** In the context of POS Tagging:

- **States ($Q$):** The hidden sequence (the POS tags).
- **Observations ($O$):** The visible sequence (the words).
- **Transition Probabilities ($A$):** Probabilities of moving from one state (tag) to the next.
- **Output/Emission Probabilities ($B$):** Probability of observing a word given a specific state (tag).

**3. The Three Main HMM Tasks:**

|Task|Goal|Algorithm|Mechanism|
|:-:|:-:|:-:|:-:|
|**1. Probability**|Calculate $P(O)$ given $\lambda$.|**Forward Algorithm**|Efficiently computes the probability by jointly considering all paths to the current state.|
|**2. Decoding**|Find the **most probable state sequence** $Q$ given $O$ and $\lambda$.|**Viterbi Algorithm**|Uses dynamic programming. Stores a pointer to the most probable previous state and follows these pointers backwards to retrieve the optimal sequence.|
|**3. Learning**|Find the optimal parameters $\lambda=(A, B)$ given $O$ and $Q$.|**Baum-Welch Algorithm**|An iterative algorithm that calculates an initial estimation of parameters and uses that to improve the estimation.|

##  **V. Model II: Conditional Random Fields (CRF)**

The Linear Chain CRF is a **Discriminative Model** that directly models the conditional probability $P(y|x)$.

**1. Discriminative Advantage:** Since the input sequence ($x$) is observed, the CRF model **does not need to model $P(x)$**. This is crucial because generative models (like HMMs) struggle when highly dependent features (like capitalization, suffix, or prefix) are introduced, often requiring harmful independence assumptions. CRFs bypass this issue, allowing the free introduction of such complex, dependent features.

**2. Model Formulation:** The CRF model is defined using feature functions ($f$) and parameters ($\theta$).

**3. Feature Functions ($f$):** CRFs use flexible feature functions to capture various relationships, including:

- **First-order dependencies:** e.g., $1(y=\text{DET}, y'=\text{NN})$.
- **Lexical features:** e.g., $1(y'=\text{DET}, x=\text{"the"})$.
- **Lexical features with context:** e.g., $1(y'=\text{NN}, x=\text{can}, \text{pre}(x)=\text{"the"})$.
- **Additional features:** e.g., binary features like $1(y'=\text{NN}, \text{cap}(x) = \text{true})$.

**4. Inference and Training:**

- **Inference (Finding the best sequence):** Despite the exponential number of possible label sequences, the linear-chain layout allows for the efficient computing of the most probable POS sequence using **dynamic programming**.
- **Training (Finding the best weights $\theta$):** The optimal weights are found using **Maximum Likelihood estimation**. Because the objective function is a convex function, gradient descent can be used to find the optimal values.

##  **VI. Summary and Comparison**

In practical POS tagging tasks, CRFs generally outperform HMMs, especially when utilizing rich input features. For instance, in one study, the baseline HMM model achieved an error rate of **5.69%**, while the improved CRF model (CRF+, using spelling features) achieved a lower error rate of **4.27%**.

---

**Conceptual Analogy: Why Discriminative Models Excel**

Think of the task as predicting whether a person will buy a certain product ($Y$) based on their characteristics ($X$).

- **HMM (Generative):** Tries to build a complete world model by predicting everything: $P(\text{Characteristics}, \text{Purchase})$. If characteristics include "age," "income," and "zip code," the HMM must model how "zip code" depends on "age," which is often complex and irrelevant to the final purchase decision itself.
- **CRF (Discriminative):** Focuses solely on the boundary: $P(\text{Purchase} | \text{Characteristics})$. It doesn't care how "age" relates to "zip code." It simply uses all relevant observed characteristics as evidence to maximize the likelihood of the outcome, making it more flexible and robust for complex feature integration.
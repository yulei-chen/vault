![[Pasted image 20251206040351.png]]

## **Module 1: The Core Challenge: Natural Language Understanding (NLU)**

**Natural Language Understanding (NLU)** is fundamentally about representing the semantics of natural language. One way to view it is as a translation from natural language to some form of representation.

**Difficulties in NLU:** NLU is challenging due to inherent properties of language:

1. **Ambiguities:** This includes lexical, syntactic, and referential ambiguities.
2. **Vagueness:** Statements lack precise definition (e.g., "I had a late lunch").

**Example Application: Spoken Language Understanding (SLU)** SLU aims to transform text into a task-specific semantic representation of the userâ€™s intention. It typically involves three subtasks:

1. **Domain Detection:** Identifying the topic of the utterance (e.g., classifying "Show flights from Boston to New York today" as "Airline Travel").
2. **Intent Determination:** Assigning a domain-dependent intent class (e.g., within "Airline Travel," the intent is "Find_Flight").
3. **Slot Filling:** Finding the attributes or slots needed for the intent (e.g., extracting Departure, Arrival, and Date for the "Find_Flight" intent). This is often solved using sequence labeling techniques (like BIO/BILOU encoding).



## **Module 2: Foundation: Transfer Learning**

Deep learning models require large amounts of data due to their many parameters.

**Transfer Learning (TL)** offers a solution by reusing parameters or knowledge learned ==from one or more source tasks on a related target task==.

**Implementation Key: Self-Supervised Learning (SSL)** To implement TL efficiently in NLP, we use Self-supervised learning (SSL), where we automatically create a supervised task from vast amounts of unlabeled data (like news and books).



##  **Module 3: Training Methods: Self-Supervised Learning (SSL)**

For NLP, SSL typically uses prediction tasks.

1. **Next Word Prediction (NWP):**
    
    - Models (such as Recurrent Neural Network (RNN) Language Models) predict the next word in a sequence.
    - **Disadvantage for Encoders:** Traditional RNN LMs only utilize the left-side context of a word.
2. **Mask Language Model (MLM):**
    
    - The model predicts missing words in the input sequence.
    - This technique enables **bidirectional processing**, allowing the encoder to use context from both the left and right sides of the word. Models using this approach are non-autoregressive.



## **Module 4: Main Pretrained Encoders**

Pretrained Text Encoders generate **Contextual Word Embeddings**, meaning the representation of a word depends on its surrounding context.

1. **ElMo (Embeddings from Language Models):**
    
    - An RNN-based model (Peters, 2018).
    - Trained using next word prediction.
    - Achieves bidirectionality by training two separate 2-layer RNN Language Models (forward and backward), and the final embedding is derived through a weighted sum of these layer representations.
2. **BERT (Bidirectional Encoder Representations from Transformers):**
    
    - A bidirectional model based on the Self-attention framework (Devlin, 2019).
    - Uses **Masking** as its core idea.
    - **Pre-training Tasks:**
        - **Mask Language Model (MLM):** Masks 15% of the subword tokens (with varying replacement strategies) and predicts the original words.
        - **Next Sentence Prediction (NSP):** Predicts whether Sentence B immediately follows Sentence A in the original text, relying on the special `[CLS]` token embedding for classification.
    - **Input Structure:** Uses subword units, where `[CLS]` is the first token (used for classification tasks), and `[SEP]` tokens separate sentences (which are identified using A/B Segment Embeddings).



## **Module 5: Model Application: Fine-tuning**

After pre-training, models like BERT are adapted to specific tasks through **Fine-tuning**.

**Fine-tuning involves three decisions regarding knowledge transfer:**

1. **What to Share:** You can share only the word embedding layer (like static embeddings) or share all layers except the final classification head (for contextual embeddings).
2. **What to Add:** New layers required for the target task, such as a Classification layer, Sequence Layer, or CRF.
3. **What to Train:** You can train only the newly added layers, or train everything (all layers in the model).

Does this structured overview make the topic clearer? Which model or concept would you like to explore next?
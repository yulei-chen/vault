<iframe width="560" height="315" src="https://www.youtube.com/embed/T05t-SqKArY?si=pLZ1Z90xMH1BHQoV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/BaM1uiCpj_E?si=jLvgne3dLZFdClui" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Motivation and Core Task

Topic models address **information overload** by helping to **Automatically process data**. Traditional NLP solutions include Summarization, Information extraction, and Question answering.

*   **Goals:** Topic models aim to **Cluster/Organize** data, **Discover hidden themes**, and **Annotate documents with themes**.
*   **Topic Definition:** A topic is characterized conceptually by **frequent words**, and formally as a **Distribution over words**.
*   **Task:** This is an ==**Unsupervised** text clustering task==.
    *   ==**Input:** Corpus and the specified **Number of Topics** ($K$)==.
    *   ==**Output:** The learned **Topics** (Distribution over words) and **Topics per document**==.

## Representation

The dataset is defined as a **Collection of documents**.

*   **Document Representation:** Individual documents are typically represented using a **Bag-of-word representation** (a $|V|$-dimensional vector, where $|V|$ is the vocabulary size).
*   **Collection Representation:** A collection of $M$ documents is represented as a **$|V| \times M$-dim matrix**.
*   **Matrix Values:** Matrix values can represent **Counts**, Indicator function, or TF-IDF.

## Latent Semantic Analysis (LSA)

LSA is a non-probabilistic approach designed to **Model which words typically co-occur** using matrix representation.

*   **Method:** The core idea is to **Perform matrix factorization**, specifically using **SVD** (Singular Value Decomposition) to yield Latent Semantic Analysis. $K$ represents the Number of topics.

## Probabilistic Models: LDA and Inference

Probabilistic models assume that the observed data is generated by a **generative probabilistic process** that includes **hidden variables**. The hidden variable in topic modeling is the **Thematic structure**. ==The main task is to **Infer hidden structure** and **Generalize to new data**==.

### Latent Dirichlet Allocation (LDA)

LDA is the primary probabilistic model used (Blei, 2003).

*   **Name Breakdown:**
    *   **Latent:** Topic is **unknown / Latent in the text**.
    *   **Dirichlet:** Refers to the **Distribution of distributions**. Documents are Distributions of topics, and Topics are Distributions of words.
    *   **Allocation:** Allocate topics to the words.
*   **Process/Intuition:** Documents **exhibit multiple topics**. Topics are Distributions over words; Documents are a **Mixture of topics**; and words are **Drawn from one of the topics**. The major challenge is that **Only documents are observed**, while topics are hidden variables.
*   **Graphical Model:** This model encodes assumptions and dependencies. Nodes represent random variables, and **Shaded nodes** ($W_{d,n}$) represent the **Observed word**. Plates indicate replications (e.g., $D$ for documents, $N$ for words within documents, $K$ for topics).
	![[Pasted image 20251126030848.png]]

### Inference

The goal of inference is to compute the **posterior distribution** conditioned on the documents: $p(\text{topics, proportions, assignments} \mid \text{documents})$.

*   **Variables to Infer:** Per-word topic assignment ($z_{d,n}$), Per-document topic proportions ($\theta_d$), and Per-corpus topic distribution ($\beta_k$).
*   **Challenge:** The posterior distribution **Cannot be computed** exactly due to the intractable marginal likelihood. Even assuming the global variable (Topic, $\beta_k$) is fixed, local inference (for $\theta_d, z_{d,n}$) remains **intractable**.
*   **Solution:** **Approximate** methods, such as **Gibbs sampling**, are necessary.

### Gibbs Sampling

Gibbs sampling is an iterative **Markov Chain Monte Carlo** method used for approximation.

*   **Procedure:**
    1.  Start with **random assignment** of all random variables (topic-word allocation, topic distribution, document distribution).
    2.  For a given word, **Keep everything except that word fixed**.
    3.  **Assign a topic to this word based on current distribution**.
    4.  Repeat for several iterations.
*   **Sampling Formula:** The probability of assigning topic $k$ to word $w_{d,n}$ is proportional to the product of two components:
    1.  **Document Likelihood:** How much document $d$ likes topic $k$ ($\frac{n_{d,k} + \alpha_k}{\sum_{i} n_{d,i} + \alpha_i}$). This depends on $n_{d,k}$ (Number of times document $d$ uses topic $k$) and $\alpha$ (Dirichlet parameter for document to topic distribution).
    2.  **Topic Likelihood:** How much topic $k$ likes word $w_{d,n}$ ($\frac{v_{k,w_{d,n}} + \eta_{w_{d,n}}}{\sum_{i} v_{k,i} + \eta_i}$). This depends on $v_{k,n}$ (Number of times topic $k$ uses word type) and $\eta$ (Dirichlet parameter for topic to word distribution).

## Quality Measures

Evaluating topic models (Unsupervised learning) differs from supervised learning.

*   **Measure 1: Held-out Log Likelihood:** A good model should assign a **High probability for real data** (held-out data).
*   **Measure 2: Word Intrusion:** This measure tests topic coherence. A user is presented with the most probable words for a topic, plus a **high probability word from another topic** (the intruder). The user is asked to find the word that does not belong, and the metric is the **Percentage of users who selected the intruder**.
*   **Note:** Evaluation metrics do not always agree; it is necessary to **Measure what you need**.
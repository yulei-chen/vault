## Morphology

### Morphology and Languages

#### 1. Isolating Languages

Example (Chinese): 我看书 — “I read/look at a book.”. Morphemes:

- 我 = I
- 看 = read/look
- 书 = book

Key idea: Minimal or no inflection; morphemes usually stand alone.

#### 2. Agglutinative Languages

Example (Turkish): ev-ler-in-de — “in your houses”. Morphemes:

- ev = house
- -ler = plural
- -in = your
- -de = in

Key idea: Morphemes have clear boundaries and single meanings, attached in sequence.

#### 3. Fusional (Inflectional) Languages

Example (Spanish): habl-o — “I speak”. Morphemes:

- habl- = speak
- -o = 1st person + singular + present

Key idea: A single morpheme typically encodes multiple grammatical features.

#### 4. Polysynthetic Languages

Example (Inuktitut): tusaatsiarunnanngittualuujunga — “I really can’t hear well”. Simplified morphemes:

- tusaa- = hear
- -tsiaq- = well
- -junnaq- = be able
- -nngit- = not
- -tualuu- = very
- -junga = I

Key idea: Many morphemes combine to express what would be a whole sentence.

#### 5. Root-and-Pattern Languages

Example (Arabic): kitaab — “book”. Morphemes:

- k-t-b = root “write”
- i\_\_a\_\_ = vocalic pattern forming a noun

Key idea: Words are formed by inserting vocalic patterns into consonantal roots.

### Concepts of morphology

- Lexeme, e.g., SEE
- Word-form, e.g., saw, seeing, saws
- Paradigm, e.g., I, my, mine, me
- Word family, e.g., act, action, actor, activate, active, activist

### Types of morphemes

#### 1. Stem
- Lexical/Content
- Functional
- Root
#### 2. Affixes
- Suffix
- Prefix
- Infix
- Circumfix

### Word Formation

#### 1. Inflection

- Changes the grammatical form of a word without creating a new lexeme.
- Does not change core meaning or word class.
- Typical categories: tense, number, case, agreement.

Examples:

- walk → walks (3sg)
- cat → cats (plural)
- see → saw (past)

#### 2. Derivation

- Forms a new word (new lexeme) by adding derivational affixes.
- Often changes meaning and sometimes word class.

Examples:

- happy → unhappy
- nation → national
- modern → modernize
- read → reader

#### 3. Compounding

- Combines two or more roots/words to form a new lexeme.
- The components remain recognizable.

Examples:

- black + board → blackboard
- sun + light → sunlight
- ice + cream → ice-cream

Here’s a concise English note summarizing the morphological processes you listed, with clear examples:

### Morphology Tasks

#### 1. Lemmatization

- Maps a word form to its lemma (dictionary/base form).
- Ignores context.

Example:

- saw → see

> Lemma and Lexeme are different. Lexeme is an abstract concept, while lemma is a concrete concept.

#### 2. Morphological Analysis

- Breaks a word into (lemma + grammatical tag).
- ==Does not consider context==, so may produce multiple analyses.

Example:

- saw → {(see, verb.past), (saw, noun.sg)}

#### 3. Tagging (Morphosyntactic Disambiguation)

- Uses context to select the correct analysis.

Example:

- Sentence: Peter saw her
- saw → {(see, verb.past)}

#### 4. Morpheme Segmentation

- Splits a word into its smallest meaningful units (morphemes).

Example:

- de-nation-al-iz-ation → de + nation + al + iz + ation

#### 5. Generation

- Produces a word form from a lemma + features.
- Essentially the reverse of lemmatization.

Example:

- (see, verb.past) → saw

## Machine Learning in NLP

Both Feature Engineering (FE) and Representation Learning (RL) are methodologies designed to solve the critical challenge in machine learning models: how to map text into a fixed vector representation, or $R^m$.

However, they differ fundamentally in their approach, timeframe, and resulting data structure.

### Feature Engineering vs. Representation Learning

#### 1. Core Philosophy and Timeframe

Feature Engineering (FE) represents the dominant methods used over the last 25 years. This approach relies on manually defined templates to describe the important information of the text. FE is often decried as “costly, hand-crafted, expensive, domain-specific,” among other criticisms. While simple features typically give the bulk of the performance and can yield high accuracies, the feature set is still difficult to maintain.

Representation Learning (RL) describes the methods developed over the last 10 years. The defining characteristic of RL is that the features are automatically learned. The result of this process is an approximation, meaning some information are lost, and the representation is typically task dependent.

#### 2. Representation Format and Structure

The methodologies lead to stark differences in the resulting vector format:

- FE commonly uses representations like the Bag of Words (BoW). This approach results in a Sparse matrix, where most entries are zero. BoW uses one feature per word, which can be a Binary feature (0 or 1) or a Count feature. Feature Importance and TF-IDF can help solve problems respectively about word importance and high-frequently used no-meaning words.
- RL generates a Neural representation, which is characterized as Dense.

#### 3. Issues of Feature Engineering

- Negation: It's **not** a great monster movie.
- Different sense: There's **a great deal of** corny dialogue and preposterous moments.
- Multiple sentiments: A great ensemble cast can't lift this heartfelt enterprise out of the familiar.
- Number of features:
  - Too few. Hard to distinguish different inputs
  - Too many. Overfitting

#### 4. Handling Language Challenges

Representation Learning evolved to better handle critical challenges inherent in text data that constrained Feature Engineering:

- Word Order and Length: FE methods like BoW ignore word position when dealing with variable length input. Although FE can use Higher-Order Binary Feature Templates (like bigrams or trigrams) to consider word order and maintain a fixed size vector representation, this introduces the challenge of an extremely large number of features. Conversely, RL must address the fact that word order is important and must handle variable length input.
- Open Vocabulary: For FE, open vocabulary is a challenge that requires the use of a fixed vocab. RL also faces the open vocabulary challenge, but aims to solve it by being able to understand **unknown words**, such as the complex term Hippopotomonstrosesquipedaliophobia.
- Granularity: RL’s approach to solving vocabulary challenges involves identifying the best granularity—choosing among words, subwords, or characters. RL acknowledges that while using characters provides much training data and allows understanding of unknown words, it leads to long sequences for text, which is not efficient. Therefore, RL uses machine learning based subword segmentation to learn efficient representation. Techniques like Byte-Pair Encoding (BPE), which starts with an Initial vocabulary of characters to ensure every words can be represented, are employed to strike this balance.


## Word Classes

**Part-of-Speech (POS)**: POS tags classify words based on their grammatical relationship and functional category (e.g., **noun**, **verb**) in context. This differs from **word sense** (meaning).

* **Classification**: Words belong to **Open classes** (lexical words) (Nouns, Verbs, Adjectives, etc.) or **Closed classes** (functional) (Determiners, Pronouns, Conjunctions).
* **Universal Tag Set**: Smaller, coarser sets (like the **12-tag Universal Tag Set**) are used for cross-lingual purposes.
* **Challenge (Ambiguity)**: Many word instances are **ambiguous**, meaning the same word type can have multiple POS tags (e.g., "back" can be **JJ**, **NN**, **RB**, or **VB**). POS tagging is the process of determining the correct tag for a word given its context.

## Syntax

Syntax studies how words organize into hierarchical units called **constituents (phrases)**. Only entire phrases, such as a **Prepositional Phrase (PP)**, can be moved within a sentence.

* **Context Free Grammar**: Introduced by **Chomsky**, CFG is a formal system **$G = (V, \Sigma, R, S)$** used to generate sentences and assign parse trees. **Non-terminals ($V$)** are phrase variables (NP, VP), and **Terminals ($\Sigma$)** are the actual words.
* **Phrase Structure Grammar**: This applies CFG rules to define natural language structures (e.g., Declarative sentences are **NP VP**).

  * **Subcategorization**: Verbs enforce constraints on the required accompanying phrases; e.g., **Transitive verbs** require a direct object, while **Intransitive verbs** do not.
  * **Ambiguity**: A sentence can have multiple correct parse trees (**Structural Ambiguity**). This includes **Attachment ambiguity** and **Coordination ambiguity**.
* **Lexicalized Grammars** (e.g., **Combinatory Categorial Grammar**): These models use a lexicon to encode complex grammatical dependencies like **Agreement**, **Subcategorization**, and **Long-distance dependencies**. Words are assigned detailed categories (e.g., the verb "Cancel" has the category **$(S\setminus NP)/NP$**).

  * **Forward rule application ($X/Y ; Y \to X$)**
  * **Backward rule application ($Y ; X\setminus Y \to X$)**
  ![[Pasted image 20251126025040.png]]
* **Dependency Grammar**: This approach describes structure using directed binary relations (**typed relations**) between a **head** and its **dependent**.

  * **Graph $G(V, A)$**

    * **$V$**: Vertices correspond to words
    * **$A$**: Arcs corresponding to relations
  * **Restrictions**:

    * Connected
    * Designated root node
    * Acyclic (==connected acyclic graph is a tree==) or planar
	 
		![[Pasted image 20251126025053.png]]


## Language Model

Language Models automatically learn sentence structure by assigning a **Probability** to a sentence.

### Challenge

Simple counting fails due to **sparse data**; many valid but unseen sentences are assigned a probability of zero.

**Solution: Chain Rule.**

$$
P(w_1 w_2 \ldots w_n) = \prod_{i=1}^n P(w_i \mid w_1 \ldots w_{i-1})
$$

E.g.,

$$
\begin{align}
P(\text{“its water is so transparent”}) &= P(its)\times P(water \mid its) \\
&\times P(is \mid its\ water) \\
&\times P(so \mid its\ water\ is) \\
&\times P(transparent \mid its\ water\ is\ so)
\end{align}
$$

### Another Challenge

Estimating the probability of a word from its **full history** is intractable.

**Solution: Markov Assumption.**
The probability of a word $w_i$ depends only on the previous **$n-1$** words.

**N-gram Models**: **Unigram**, **Bigram**, **Trigram**, etc. Though insufficient for capturing **long-distance dependencies**, they are practical.

![[Pasted image 20251126025309.png]]



### Maximum Likelihood Estimation (MLE)

MLE estimates N-gram probabilities by **maximizing the likelihood** of training data.

![[Pasted image 20251126025315.png]]

A key challenge persists: if an N-gram is an **unseen event (count = 0)**, its probability is **0**, making the entire sentence probability **0**.



If you'd like, I can also format these into a printable PDF, add color themes, or convert into study flashcards.

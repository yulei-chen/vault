NLP has undergone a **major transformation** over the past few decades, shifting from **traditional statistical methods** to **modern neural network-based approaches**.

Early NLP systems relied heavily on **statistical language models (SLMs)**. These models estimate the probability of a word sequence based on observed frequencies in large text corpora. **N-gram models** is one of the most common approaches, which predict the next word given the previous n−1 words. For example, a **trigram model** predicts the next word based on the previous two words.

However, SLMs have limitations about **data sparsity**, **context window**, and **feature engineering** (SLMs often required manual preprocessing and feature extraction).

In the part, we will focus on **neural network based methods**.

## Word Embeddings

Word Embedding is one of **Dense Embeddings**. Dense vector has the following advantages:

* **Short vectors** may be easier to use as features in machine learning (less weights to tune)
* **Dense vectors** may generalize better than storing explicit counts
* They may do better at capturing ==synonymy==:

  * car and automobile are synonyms; but are distinct dimensions
  * a word with car as a neighbor and a word with automobile as a neighbor should be similar, but aren’t
* In practice, they appear to **work better**

So can we convert **one-hot (Sparse)** to **Dense Embeddings**? Assume there is a matrix **W**. We can ==multiply our one-hot with **W**==. This gives us the “embedding” for a word. **W is what we need to learn** (in the context of machine learning).

## Language Models

> Please read Q&A for more details.

* **Word2Vec**

  * **CBOW**
  * **Skip-gram**

    * **Negative sampling**
* **Fasttext**

  * character representation
* **Glove**

## Q&A

**Q: What is the relationship between the corpus-based approach and feature engineering?** A: In traditional Corpus-based Approaches to NLP, feature extraction is an explicit process. This feature extraction defines the features used by the statistical model, such as the word ID, case, and POS tags (e.g., Word: 546; Case: Upper-case; POS: NN). In Deep Learning Approaches, the input text still undergoes feature extraction, but the output is a dense binary vector (e.g., 10010010010101). Therefore, feature engineering is the traditional method of explicit feature extraction from the corpus, which has been largely replaced by **automatic feature learning** in deep learning.

**Q: What is an important property of word embeddings?** A: An important property of word embeddings is that **semantic similar words should have similar representation**. This concept is tied to J.R. Firth’s 1953 dictum: “You shall know a word by the company it keeps!”.

**Q: What determines the size of one-hot embeddings?** A: The size, or dimension, of a one-hot encoding vector is determined by the **vocabulary size**.

**Q: What is an advantage of dense embeddings?** A: Dense vectors have several advantages: they are **short vectors**, may **generalize better**, and appear to **work better in practice**, especially at **capturing synonymy**.

**Q: What is a problem with n-gram models?** A: A major problem with statistical N-gram models is that most longer word sequences have **not been seen in the corpus**. This requires a non-satisfactory solution like back-off to a shorter sequence. N-gram models also operate in **Discrete Space**.

**Q: Why do we call the Feed Forward Neural Network here self-supervised?** A: The Feed Forward Neural Network language model is considered **self-supervised** because training needs only unlabelled data (pure text). The model is trained to predict the next word given previous words. The **meaning of the word** is important for estimating the next word, and this meaning needs to be encoded in the embedding.

**Q: What is the advantage of a continuous space?** A: The advantage of mapping words to a **continuous word representation** is generalization to similar n-grams. Unlike discrete space, which only allows for exact matching, continuous space enables the model to automatically learn optimal features and find the nearest neighbor, capturing **syntactic similarities** and **semantic similarities**.

**Q: Is the position of the input words important?** A: For the Continuous Bag of Words Model (CBOW), the position of the input words is **ignored**. The model sums the surrounding words to predict the center word.

**Q: How many embeddings do we get for each word?** A: For Word2Vec models (CBOW and Skip-gram), typically there is **one embedding for each word**.
> More details: 
> - [Learning Word Embedding](https://lilianweng.github.io/posts/2017-10-15-word-embedding/) 
> - [Word2Vec (CBOW, Skip-gram) In Depth](https://medium.com/@fraidoonomarzai99/word2vec-cbow-skip-gram-in-depth-88d9cc340a50)

**Q: What is the CBOW score function based on?** A: The CBOW score function is based on the **dot product** to measure similarity.

**Q: What is the difference between CBOW and Skip-gram?** A: CBOW and Skip-gram have opposite objectives:

* **CBOW predicts the word based on surrounding words**.
* **Skip-gram predicts the surrounding words given the current word**.

**Q: What is maximized with the Skip-gram objective?** A: The Skip-gram objective is to **maximize the probability** of any context word given the current word. This objective uses a **negative log loss**.

**Q: Why is negative sampling necessary?** A: Negative sampling is necessary because the standard Skip-gram objective requires **normalization over the entire vocabulary**, which is computationally demanding. Negative sampling resolves this by generating samples and summing over those samples instead of the entire vocabulary.
<iframe width="560" height="315" src="https://www.youtube.com/embed/viZrOnJclY0?si=7xOSpsw4-2geAk0_" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

**Q: Why do we use sigmoid instead of softmax in the objective function?** A: We use the **Sigmoid function** ($\sigma$) because negative sampling converts the problem from a multiclass classifier to a **binary classifier**.

**Q: Where do we get negative examples from?** A: Negative examples are sampled from the **lexicon**. The negative set typically contains 2 to 20 words sampled from some distribution, such as uniform, unigram, or a **smoothed unigram**. The smoothed unigram distribution is created by raising the probabilities to the power of $\frac{3}{4}$ and then renormalizing.

**Q: For a corpus with fixed size, are there more unique 6-grams or 3-grams?** A: Based on the problem of N-gram models—that most longer word sequences have not been seen—it can be inferred that there would be **more unique 6-grams** than 3-grams, as longer sequences lead to higher sparsity.

**Q: How are context and target words represented with FastText?** A: FastText uses **character N-grams**:

A: FastText uses character N-grams:

* The word embedding for the target word is the **sum of character 3-grams to 6-grams and the whole word itself**.
* The word embedding for the context word is taken from the embedding table **without adding character n-grams**.

**Q: What is the difference between Word2Vec and FastText?** A: FastText is an extension of Word2Vec that addresses challenges found in Word2Vec. FastText uses **subword representation (character n-grams)** to solve issues in morphologically rich languages and to create representations for words that have not been seen previously.

**Q: What is a key difference between Word2Vec and GloVe?** A: The key difference lies in the perspective used for training: Word2Vec uses a **Local view** (predicting words within the context), while GloVe uses a **Global view**. GloVe leverages **Word co-occurrence matrices** and is motivated by the **ratio of co-occurrence probabilities**.

**Q: In which situations does GloVe likely produce unreliable word embeddings?**

* **Rare words** – too few co-occurrences to learn meaningful vectors.
* **Small corpus** – global statistics are sparse and noisy.
* **Domain mismatch** – training domain differs from the target domain.
* **Polysemous words** – one vector cannot represent multiple senses.
* **No subword modeling** – fails on morphology and misspellings.
* **Out-of-vocabulary words** – no embeddings for unseen words.
* **Biased training data** – co-occurrence statistics reflect and amplify bias.


## I. Sequence-to-Sequence (Seq2Seq) Models Overview

### A. Core Concept and Tasks

Seq2Seq models are designed for tasks where both the input and the output sequences can be of variable length.

|Task Type|Description|Examples|
|:--|:--|:--|
|**Seq2Seq**|Variable length input results in variable length output.|Summarization, Question-Answering (QA), Dialog, and Machine Translation.|
|**Sequence Labeling**|Variable length input results in a label sequence of the same length.|Named Entity Recognition. (Note: Some QA can be modeled as sequence labeling, identifying the answer within the document).|

### B. Summarization (Key Application)

The primary idea of summarization is to generate a short version of the original text.

1. **Applications** Summarization can be applied to diverse texts, including news articles, email threads, health information (like clinical reports), and meeting summarization (e.g., creating automatic meeting minutes).
    
2. **Scope Types**
    
    - **Single-document summarization:** Generates an abstract, outline, or headline from a single document.
    - **Multiple-document summarization:** Summarizes information from a group of documents, such as a series of news stories on the same event.
3. **Focus Types**
    
    - **Generic summarization:** Summarizes the document's content generally.
    - **Query-focused summarization:** Summarizes the document specifically concerning user queries (i.e., answering a question by summarizing the relevant content).
4. **Generation Types**
    
    - **Extractive summarization:** Selects a subset of existing text segments, involving only deletion operations (e.g., sentence or key-phrase extraction). This method is simpler but generally less grammatical or fluent.
    - **Abstractive summarization:** Uses natural language generation to rewrite the inputs, producing a summary that is more human-like. Sentence Summarization/Compression is an example of partial abstractive rewriting.

---

## II. Conditional Text Generation and Model Architectures

Text generation aims to predict the probability of the next word, typically using architectures like RNN or Transformer. In **Conditional Text Generation**, the output must depend on the input (e.g., the translation should have the same content as the source). The next word depends on previous output words _and_ the input.

There are two main approaches to modeling Seq2Seq tasks:

### A. Decoder-Based Models (Decoder Only)

This approach modifies the input data structure while keeping the model the same.

- **Mechanism:** The input sequence is prepended to the target sequence. The combined sequence is processed by an autoregressive language model.
- **Input Handling:** The original input acts as a prefix to the sequence being generated.
- **Architecture:** The model typically uses a recurrent architecture (RNN) or a unidirectional Transformer with Masked Attention. Since the model only attends to previous states, it uses **Masked Attention** to ensure future words are not used in predicting the current word.
- **Generation Property:** These models use autoregressive generation of both input and output sequences.

### B. Encoder-Decoder Models

This architecture uses two separate components to process the input and generate the output, suitable for tasks like machine translation.

1. **Components and Information Flow**
    
    - **Encoder:** Reads in the source sentence and represents its content, typically as a hidden vector with a fixed dimension. The encoder usually operates **bidirectionally**. In training, output probabilities are generally _not_ needed from the encoder stage.
    - **Decoder:** Generates the target sentence word by word. The decoder is typically **unidirectional** (autoregressive) and depends on the previous predictions and the source sentence (input). Information transfer originally relied on the encoder's last hidden state.
2. **Training Process** Training involves calculating the forward path (determining hidden state activations). Error is measured using a cross-entropy loss function, calculated only at the decoder positions. The error is then backpropagated from all decoder positions to update the weights.
    

---

## III. Encoder-Decoder Attention Mechanism

### A. Cross-Attention (Encoder-Decoder Attention)

Cross-Attention is the mechanism used to efficiently transfer information from the Encoder to the Decoder. Instead of relying only on the final hidden state of the encoder (which struggles with long-range dependencies), attention allows the decoder to create a new, targeted summary of the encoder states at each generation step.

1. **Inputs:**
    
    - **Query (Q):** Provided by the current Decoder State.
    - **Key (K) / Value (V):** Provided by the Encoder States.
2. **Calculation Steps**
    
    - **Similarity Measurement (Energy Function $e_{ij}$):** Measures the similarity between the Query (target state) and the Key (source state). Methods include Scaled Dot Product (often scaled by $\frac{1}{\sqrt{key}}$ to stabilize training), Multiplicative, or a Feed Forward Network.
    - **Normalization:** The calculated energy scores are normalized using the $\text{softmax}$ function to create an attention probability distribution, helping to prevent vanishing/exploding gradients.
    - **Context Vector Generation:** The attention scores (A) are used as weights to take a weighted sum of the Value vectors (V), resulting in a single context vector.
3. **Integration into Decoder** The Decoder uses two primary inputs to generate the next word: the last outputted target word embedding and the context vector. Various methods exist for integration, such as the Conditional GRU (Bahdanau, 2014) or the Luong approach (which combines RNN output and the Context vector before softmax).
    

### B. The Transformer Architecture

The Transformer is a specific Encoder-Decoder model introduced in 2017.

- **Encoder (Source Sequence):** Consists of Self-Attention layers that operate **bidirectionally**.
- **Decoder (Target Sequence):** Contains three main components:
    1. **Masked Self-Attention:** Only attends to previous states/the left (unidirectional), simulating the sequential decoding process.
    2. **Encoder-Decoder Cross-Attention:** Connects the encoder to the decoder. The Key vectors come from the encoder, and the Query vector comes from the current decoder state.
    3. **Feed-Forward Network (FF) and Output Layer.**
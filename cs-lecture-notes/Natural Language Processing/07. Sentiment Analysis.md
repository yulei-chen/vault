![[Pasted image 20251126031330.png]]
## Part I: Task Definition & Challenges

### 1.1 Sentiment Analysis Tasks

Sentiment Analysis takes a **text sequence with variable length** as input and outputs a **sentiment label**. Tasks range from narrow to broad:

- **Polarity classification**.
- **User rating** (e.g., 1 to 5 stars).
- **Feature/aspect-based sentiment analysis**.
- Agreement detection.
- Subjectivity/viewpoint detection.

### 1.2 Challenges

|Type|Description|
|:--|:--|
|**Linguistic**|**Context dependency** (e.g., meaning changes between a book vs. movie review), **Different aspects** within the text (e.g., "The phoneâ€™s camera is great, but its battery life is terrible"), **Negation handling** (e.g., "The movie was not bad"), and Co-reference resolution.|
|**Machine Learning**|**Variable length sequences** require neural networks to output a fixed-size representation. **Sequence order is important**, with the end of the text often being more critical. Limited training data is also a factor.|

---

## Part II: Deep Learning Architecture Overview

The basic Deep Learning Model uses **End-to-end optimization** because the **Whole network** is **differentiable**.

The model generally consists of three main stages:

1. **Encoder**: Function to represent a **sequence of words as a vector**.
2. **Feed Forward Network (FFN)**: Provides a **better representation of the sentence content**.
3. **Classification Layer**: Outputs the final sentiment label.

---

## Part III: The Encoder Components

The Encoder is composed of three components: **Word Embedding**, **Sequential Layers**, and the **Aggregation Layer**.

### 3.1 Input Representation: Word Embeddings

- **Word Embeddings** map a word to a **dense vector**.
- This layer often holds the **largest number of features in the whole network**.
- Contrast with One-hot encoding: One-hot encoding results in a large number of features $|V|$ and makes all words equally similar.
- **Training/Initialization:** A common strategy is to use a **pre-trained model** with **fine-tuning**.

### 3.2 Sequential Layer (The Need for Context)

- A "no layer" approach, or **Bag-of-word representation**, fails because **Word order does not matter** (==e.g., "Mary loves John" vs. "John loves Mary"==).
- Since context is important, a model is needed to **Process word separately AND Keep the context**.
- Architectures used for sequential context include **Recurrent neural networks (RNNs)**, **Convolution neural networks (CNNs)**, and **Attention networks**.

### 3.3 Aggregation Layer

This layer addresses the challenge of converting **Variable input size** to **Fixed output size**. Common methods include:

- **Average word embeddings (meanpool)**.
- **Sum word embeddings**.
- **Max pooling** ($h(x)[j] = \max_{1 \le i \le n} \text{emb}(x_i)[j]$ for the $j$-th dimension).
- **Last element**.

---

## Part IV: Core Sequential Models

### 4.1 Recurrent Neural Networks (RNNs)

- **Unit:** The Vanilla RNN unit takes the **Current input $x_t$** and the **Previous hidden state $h_{t-1}$**.
- **Properties:** RNNs **Store history in a vector** and capture **Global context**.
- **Training:** Involves **Unfolding the network over time** (weights are shared across time steps) and using **Backpropagation through time (BPTT)**.
- **Issues:** RNNs suffer from **Vanishing gradients** and **Exploding gradients** (often controlled with clipping).

### 4.2 Long Short-term memory (LSTM)

An **Advanced RNN Architecture** motivated by the need for a **Memory cell**.

|Gate|Idea / Function|Inputs|
|:--|:--|:--|
|**Forget Gate ($F_t$)**|Remove information from cell $C_{t-1}$.|Current input $X_t$ and Previous hidden state $H_{t-1}$.|
|**Write/Input Gate ($I_t$)**|Add new information ($\tilde{C}_t$) to the cell. Resolves input weight conflict.|Current input $X_t$ and Previous hidden state $H_{t-1}$.|
|**Output Gate ($O_t$)**|Read information from cell $C_t$ to compute the new state $H_t$. Resolves output weight conflict.|Current input $X_t$ and Previous hidden state $H_{t-1}$.|

### 4.3 Convolutional Neural Networks (CNNs) in NLP

- **Mechanism:** Uses **1D convolution along the time dimension**, also known as **Time-delayed neural networks (TDNN)**.
- **Filters (Kernels):** Act as **feature extractors**. A filter is a vector in the word vector space that **matches a particular region of the space**. Filters can be used to capture Uni-gram, Bi-gram, or Tri-gram features (depending on kernel size).
- **Properties:** CNNs exhibit **Shift Invariance** and capture **Local context**.
- **Output:** An **Aggregation layer** (e.g., max-pooling) is used to **Reduce to fixed length**.
- **CNNs vs. RNNs:** CNNs view context **local** to the filter width, while RNNs generally look **globally**.

---

## Part V: Final Output and Training

### 5.1 Feed Forward Network & Classification Layer

- **Feed Forward Network (FFN):** Input is a **Fixed-size representation of the sentence**; Output is a **Fixed-size representation**. The parameters are the **Weights of edges** between nodes.
    
- **Classification Layer:**
    
    - **Binary Classification:** Uses a **Single Neuron** with the **Sigmoid** activation function.
    - **Multiclass Classification:** Uses **One neuron per class** with the **Softmax** activation function (to produce a probability distribution) and **Cross-entropy** as the loss function.

### 5.2 Training Strategies

Since the whole network is differentiable, training uses **End-to-end optimization**. Strategies depend on data and model:

- Random initialization.
- Frozen pre-trained model.
- Pre-trained model with fine-tuning.
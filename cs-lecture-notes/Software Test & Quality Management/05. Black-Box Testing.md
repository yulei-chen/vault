

## I. Dynamic Testing in General

**Dynamic Testing** involves testing the resulting processes of a test object (Program / System under Test, SUT) by executing and "interpreting" it. This requires supplying input values and observing the output values. This contrasts with static testing, which examines static descriptions and artifacts of the development process (like source code or models).

**Dynamic Testing Procedure:**

1. **Design Tests:** Investigate test requirements by analyzing test-related documents to determine _what_ must be tested and under _what conditions_. This step enables traceability between specifications/requirements and test conditions, and allows for impact analysis and examination of current test coverage.
2. **Specify Test Cases:** Create and describe test cases and test data using test design approaches. Expected results must include outputs, changes of data and states, and all other ramifications of testing.
3. **Specify Test Process Specification:** Arrange test cases in executable order, taking pre- and post-conditions into account. This defines the order of tests and allows for automation.
4. **Test Execution Planning:** Define the concrete order of execution, using criteria like regression testing, prioritization, and logical dependencies.

## II. Black-Box Testing vs. White-Box Testing

Test design techniques are broadly categorized into Specification-based (Black-Box), Structure-based (White-Box), and Experience-based techniques.

|Feature|Black-Box Testing|White-Box Testing|
|:--|:--|:--|
|**Input Derivation**|Derived _without_ knowledge about internal program logic.|Derived _with_ knowledge about internal logic.|
|**Design Approach**|Specification-based/Specification-oriented.|Structure-based.|
|**Focus**|Functionality, models, and requirements.|Internal code structure (e.g., `int ggT(int m, int m) {...}`).|
|**Control/Observation**|Execution is controlled externally via input values (Point of Control - PoC). Behavior is monitored externally (Point of Observation - PoO).|PoC and PoO exist internally.|

The primary focus of black-box testing is the **functionality** of the test object, which holds the greatest priority for the final product.

## III. Specification-Based Black-Box Testing Techniques

These techniques use models and requirements to derive test cases. Functional testing is a synonym where test cases are derived using the functional system specification. Functionality is defined by ISO 25010 as Completeness, Correctness, and Adequacy.

### A. Equivalence Classes (EC)

**Basic Idea:** The domain of input and output values is partitioned into Equivalence Classes. All values within a single EC are assumed to lead to the same behavior. A single arbitrary value is chosen from each class as a representative sample.

**Procedure for Defining ECs:**

- **Domain Constraint (e.g., range 1 to 100):** Create one valid EC (1 <= x <= 100) and two invalid ECs (x < 1 and x > 100).
- **Min/Max Number Constraint:** Create one valid EC and two invalid ECs.
- **Set of Values (treated differently):** Create one valid EC for every specified value, plus one overall invalid EC for everything else (e.g., Badminton vs. Football).
- **Situation Fulfillment Constraint:** Create one valid EC and one invalid EC.

**Test Case Minimization and Failure Masking:**

- The minimal criteria require at least one representative of each EC to be included in at least one test case.
- Combining representatives of invalid ECs should be avoided because it can lead to **potential failure masking**.
- Test End Criteria: **EC-Coverage** is calculated as (No. of tested EC / Total no. of EC) * 100%.
- _Advice:_ For selecting powerful test data, combine EC with failure-oriented methods like boundary value analysis.

**Combinatorial Interaction Testing (CIT):** CIT aims to minimize the test cases while covering combinations of ECs.

- **1-wise testing:** Each EC is part of at least one test case.
- **Pairwise (2-wise) testing:** Covers all 2-way combinations of variable ECs.
- The **Greedy Set-Cover Algorithm** can be used to calculate a minimum number of test cases required to cover all pairs.

### B. Boundary Value Analysis (BVA)

**Basic Idea:** Failures often occur at boundaries (where a constraint just holds or just fails to hold), leading to "off by one" errors. BVA tests these specific boundary values to increase the likelihood of finding failures.

![[Screenshot 2025-12-04 at 04.32.17.png]]

**Procedure:**

- BVA is best used in combination with Equivalence Classes.
- Test the lowest (`l`) and greatest (`g`) value of an EC.
- Extend the values to include inputs right and left of the boundary (e.g., valid and invalid values near the threshold).
- Always test two values for every boundary: the boundary value itself, and the value immediately before/after the boundary.
- For set-based domains (e.g., data structures), test the smallest and biggest valid quantities, and the smallest and biggest invalid quantities.

**Assessment:**

- **Pro:** Boundaries often contain more failures than values within the class.
- **Contra:** Defining all boundary values can be very complex, and selecting test data requires creativity without a clear "recipe".
- Test End Criteria: **BV-Coverage** is calculated as (No. of tested BV / Total no. of BV) * 100%.

### C. Decision Table Testing (DT)

**Applicability:** DT is applicable for complex rules and system requirements involving logical conditions, particularly in business processes.

**Structure and Components:**

- **Conditions:** Possible states of objects (can be True 'T', False 'F', or Insignificant '-').
- **Rules (Columns):** Combinations of condition values.
- **Actions:** Activities executed based on the rules (marked with 'X' for execute).
- **Action Pointer:** Allocates conditions with actions.

**Analysis and Test Cases:**

- The decision table is **complete** if $2^n$ combinations are defined for $n$ conditions.
- The DT is **redundancy free** if specific conditions lead to different actions.
- The DT is **consistent** if the same conditions lead to the same actions.
- **Test Cases:** Every column (rule) in the decision table corresponds to one test case.

**Assessment:**

- **Pro:** Systematically formulates logical coherences and derives condition combinations that might not be executed otherwise. DTs are easily checked for redundancy, consistency, and completeness.
- **Contra:** DTs become confusing if there are too many conditions.

## IV. Other Black-Box Approaches

1. **Use Case-Testing:** Use cases describe a continuity of interactions between actors that result in a specific outcome. Test cases are derived from process operations based on the systemâ€™s actual usage, revealing failures in those processes. Useful for system/acceptance testing.
2. **Cause-Effect-Graphs:** A graphical method describing logical casual-correlations between **Causes** (inputs, data content) and **Effects/Reactions** (outputs, resulting system states).
3. **Classification-trees**.
4. **Syntax Testing:** Used when input syntax is formally specified. Test cases are derived from syntactical rules to test both compliance and violation. Mutation techniques (such as replacing, omitting, or adding elements) are often used to test invalid syntax.

## V. Assessment of Black-Box Testing

Black-box testing methods are founded on the system's requirements and specifications. Consequently:

- They cannot detect missing or incorrect requirements or specifications.
- Detection of non-specified functionality (additional, non-required functions) is likely only found by coincidence.
- Black-box techniques are prioritized because they focus on the functionality of the test object, which is paramount for the final product.
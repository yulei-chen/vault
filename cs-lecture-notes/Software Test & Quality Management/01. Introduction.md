
## Motivation and Software Failures
Software frequently fails, often resulting in severe consequences.
*   Examples of software failures include: **22 people were wrongly arrested in Australia** due to failures in a new NZ $54.5 million courts computer system. A software failure caused **12 hours of delays** for UK fliers. Toyota recalled the Newest Priuses over software. Elon Musk's Starlink experienced a global outage due to software failure.
*   **Expensive Historical Failures:**
    *   **NASA Earth Observation Satellites (1979-1985):** The **Ozone hole was not detected for 7 years**. The reason was a software failure that interpreted the change of the ozone layer as sensor drift, which was then automatically averaged out by zero point correction.
    *   **Ariane 5 (1996):** The rocket experienced self-destruction 39 seconds after takeoff on its maiden flight. The failure was caused by reusing the position control software of Ariane 4 without testing, which led to a conversion error.
    *   **Gemini V Manned NASA Space Capsule:** Missed its landing spot by **160 kilometers** because the software did not consider the earth’s rotation around the sun.
*   **Origin of the Term "Bug":** The term originated from a **moth found in the Mark II computer** which caused an error.

## Terminology
![[Screenshot 2025-12-01 at 02.02.01.png]]

| Term                | Definition                                                                                                                                                                                                                          | Causal Chain                                                     |
| :------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------- |
| **Error**           | **Human action** that produces an incorrect result [16, IEEE 610].                                                                                                                                                                  | Error (by a human) $\rightarrow$ **Fault** (in a program).       |
| **Fault or Defect** | A flaw in a component or system (e.g., incorrect statement or data definition). It is the **Non-fulfillment of a defined requirement**, or a deviation between actual behavior and intended behavior (specification, requirements). | Fault, if encountered during execution, may cause a **Failure**. |
| **Failure**         | Deviation of the component or system from its expected delivery, service, or result [17, after ISO 24765].                                                                                                                          |                                                                  |
| **Shortcoming**     | A defined requirement or a justified expectation is **not fully realized**. For example, the usability is interfered with while the functionality has been fully realized.                                                          |                                                                  |
| **Testing**         | A process (static and dynamic) involved in the planning, preparation, and evaluation of software. Goals include checking conformance to requirements, showing if it fulfills its purpose, and **finding possible faults**.          |                                                                  |
| **Verification**    | Checking if the results of a development phase are according to the presets of the phase input documents. Question: “Did we **realize the system correctly**?” [19, ISO 9000].                                                      |                                                                  |
| **Validation**      | Checking if the developed system satisfies the individual requirements in terms of an intended usage. Question: “Did we develop the **right system**?” [19, ISO 9000].                                                              |                                                                  |
| **Fault Masking**   | An occurrence in which one defect prevents the detection of another [16, IEEE 610].                                                                                                                                                 |                                                                  |

## Software Quality

###  Definitions and Requirements
*   **Quality:** The degree to which a component, system, or process meets specified requirements and/or user/customer needs and expectations [20, ISO 24765].
*   **Software Quality:** The totality of functionality and features of a software product that bear on its ability to satisfy stated or implied needs [20, After ISO 9126].
*   **Quality Requirements:** These are part of the **non-functional requirements** in the functional specification document. Not all quality properties can be fulfilled equally (e.g., a tradeoff may exist between efficiency and maintainability), so priorities must be defined.

### ISO 25010 Product Quality Characteristics
Software quality is measured across eight main characteristics:
1.  **Functionality:** The presence and appropriateness of functions for specified tasks. Includes Suitability, Accuracy (delivering the right results with the needed precision) [22, ISO 9126], and Completeness.
2.  **Reliability:** The degree to which a system performs specified functions under specified conditions for a specified time [23, after ISO 25010]. Includes Maturity, Fault Tolerance (maintaining performance after a failure), and Recoverability (re-establishing performance and recovering data).
3.  **Usability:** Characteristics referring to the effort needed to use the software and the individual rating by users. Includes Understandability, Learnability, and Operability.
4.  **Security:** The suitability to prevent unauthorized access, protecting information and data based on authorization levels [27, ISO 25010].
5.  **Compatibility:** The degree to which a system can exchange information with others. Includes Interoperability (suitability to collaborate with preexisting systems) [28, ISO 25010].
6.  **Performance Efficiency:** The degree to which a system uses time, resources, and capacity when accomplishing its designated functions [33, ISO 25010].
7.  **Changeability (Maintainability):** The effort required to implement specified modifications. Includes Analyzability, Modifiability, Stability (risk of unexpected effects of changes), and **Testability** (effectiveness and efficiency of testing the component/system) [32, ISO 25010].
8.  **Portability:** The suitability of the software to be ported from one environment to another. Includes Adaptability, Installability, and Replaceability.

### Software Quality Assurance (SQA)
![[Pasted image 20251201022626.png]]
SQA involves **Constructive** measures (Guidelines, Models) and **Analytical** measures (Audits, Testing).
*   **Static Testing:** The program **is not executed** (e.g., Reviews, Static Analysis).
*   **Dynamic Testing:** The program **is executed** (e.g., White-Box Testing, Black-Box Testing).
*   **Testing’s Role:** Testing measures software quality and indirectly increases it by ensuring failures are detected and fixed before release.

## Fundamentals of Testing

### Testing Limitations
*   **Inability to Prove Absence of Bugs:** "Program testing can be used to show the presence of bugs, but never to show their absence!" (Edsger W. Dijkstra, 1970).
*   **Complete Testing is Not Possible:** For a program accepting three 16-bit integer inputs, performing a full test (testing $2^{48}$ possible combinations) at a speed of 100,000 combinations per second would take about **90 years**. This demonstrates that even a very simple program has many potential test cases.

### Testing Principles
*   **Start as Early as Possible:** Testing is not just a late phase; the earlier a failure is found, the less the costs to fix it.
*   **Clustering of Failures:** A small amount of modules often holds a high rate of overall failures (the **80:20 rule**). Test effort should be proportional to the expected failure density.
*   **Pesticide Paradox:** Simply repeating tests brings no new information; tests must be checked, modified, and updated. Repetitions have no efficiency.
*   **Fallacy:** Assuming that finding no failures means the software is viable is a fallacy, as the software may still fail to fit the users' expectations.

### Testing Effort and Priorities
*   **Effort in Reality:** Testing effort typically constitutes **25% - 50% of the total development effort**. Successful testing (finding failures) reduces the overall development costs.
*   **Prioritizing Test Cases:** Testing is restricted by resources, especially time. Test cases must therefore be prioritized.
*   **Prioritization Basis:** Prioritization can be based upon **Expected failure severity**, **Probability of occurrence**, or the combination of both (Risk = failure severity \* probability of occurrence). Priorities must be modifiable.

## General Testing Process

The General Testing Process is **tightly interconnected with software development** but remains an independent process. This process must be **properly adapted for every testing phase**. Every phase requires a specific testing schedule, and testing must be split into small tasks or activities.

**Contextual Factors Affecting the Process:**

- Used software development model.
- Possible testing level and test types.
- Product and project risks.
- Business domain.
- Company restrictions (budget, resources, deadlines, contractual and regulatory requirements).
- Guidelines and practices of the company, and stipulated internal and external standards.

**The Activities (Cyclical Flow):**

The process involves Test Planning, Test Analysis, Test Design, Test Implementation, Test Execution, Test Completion, and continuous Test Monitoring and Control. Some activities are executed in parallel.

![[Screenshot 2025-12-01 at 03.45.29.png]]

|Activity|Key Actions|Work Results / Documentation|
|:--|:--|:--|
|**Test Planning**|Definition of a testing strategy (goals, scope, risks, approach). Defines resources (team, environment). Specifies test granularity, test end criteria, degree of testing coverage, and scheduling.|Documented in one or more **test plans**. The plan contains test goals, referenced documents, approach, report, dependencies, and end criteria. It includes the test basis information for traceability and end criteria for monitoring.|
|**Test Monitoring and Control**|Continuous comparison of actual progress with the plan using metrics and defined end criteria (Monitoring). Adopts measures to reach goals (Control). Measures/analyzes results, controls/documents coverage, initiates corrective actions, reschedules testing, reallocates resources, and makes decisions.|Different kinds of **test reports** (progress report, summary report). Possible end criteria include the number of failures found (by degree) and comparing found vs. fixed failures.|
|**Test Analysis**|Answers: **“What is tested?”**. Specifies concrete test conditions/criteria based on documents specifying requirements (the test basis). Reviews the test basis (requirements, architecture, etc.). Defines and prioritizes test conditions for every identified feature. Establishes bidirectional traceability between features and related test conditions.|Defined and **prioritized test conditions**. Each condition must be traceable to specific covered elements in the test basis.|
|**Test Design**|Answers: **“How is it tested?”**. Specifies and prioritizes **test cases** and test sets. Identifies necessary test data and designs the test environment. Implements traceability between requirements and test cases. Test cases consist of Pre-condition, Test Input, Expected Result, and Post-condition.|**Test cases and test sets** to cover test conditions. Abstract test cases (without concrete values) are helpful for documenting test scope. Design and/or identification of test data and the test environment.|
|**Test Implementation**|Answers: “Is everything ready for the test execution?”. Implements and prioritizes **test procedures**. Combines test cases into **test suites**. Orders test suites within a **test execution schedule**. Implements test environment, prepares test data, verifies/updates traceability, and implements automation scripts.|Test procedures, test suites (where the post-condition of one test case is the pre-condition of another), and the test execution schedule.|
|**Test Execution**|Executes test cases (manually and automated) according to the schedule. Executes tests for main functionality first (if these fail, further testing is not useful). Compares expected results with actual results and documents/analyzes failures. Retests failed test cases after fixes. Determines severity and priority for correction. Checks if the test end is reached and coverage criteria are met.|Documentation of status of test cases (e.g., passed, failed, blocked). Defect report (if necessary). A **test log** detailing which components, when, by whom, how intensive, and with which results were tested.|
|**Test Completion**|Collects data and experiences from previous phases. Checks finished deliverables. Completes and **archives** the test environment, data, and infrastructure for reuse. Closes failure-tickets or creates change requirements. Documents system approval. Detects possibilities to improve the testing process.|**Test summary report**, lessons learned, and finalized testware.|

**Traceability:**

Traceability is the degree to which a relationship can be established between two or more work products (ISO 19506).

- **Vertical traceability:** Tracing requirements through layers of development documentation to components.
- **Horizontal traceability:** Tracing requirements for a test level through the layers of test documentation (e.g., test plan, test design specification, test procedure specification).

Traceability supports evaluation of test coverage, impact analysis of requirements, increase comprehensibility of reports, and IT-Governance-criteria.


## In-the-loop Processes

Testing in an automotive/Cyber-Physical Systems (CPS) context is complex because software runs on distributed Electronic Control Units (ECUs). Testing must cover both software and hardware, necessitating new steps to check integration of software onto hardware, hardware with hardware, etc.. This is often realized using **“in-the-loop”** processes.

|Process|Description and Characteristics|
|:--|:--|
|**Model-in-the-loop (MiL)**|The system is modeled entirely in a modeling tool (e.g., MATLAB/SimuLink, ASCET). **No real hardware or native code is necessary**. Sensor and actuator inputs and outputs are directly modeled. Allows modeling different scenarios to check software behavior.|
|**Software-in-the-loop (SiL)**|Supports the development and testing of ECU functions. Simulates the ECUs' hardware and software, as well as aggregate behavior. **Does not require special hardware**. The developed software model is compiled into the target platform’s language (e.g., C). The resulting executable code runs together with the simulated model on the implementation environment.|
|**Hardware-in-the-loop (HiL)**|The **system under test (e.g., an ECU) is integrated into an HiL-Simulator**. The simulator imitates the missing ECUs. It allows real-time simulation of scenarios, and the software runs in real-time. Input and output signals are sent and received by the simulator. This permits lab-based testing without needing real aggregates (like an engine ECU).|
|**Vehicle-in-the-loop (ViL)**|Connects **real hardware (vehicle, ECUs)** with virtual reality. Traffic and environment are completely simulated. The vehicle does not have to be tested in real traffic. ECUs can react to simulated sensor models.|



## Test Cases and Test Oracles

### Test Case Criteria

Test cases can be categorized based on their intended goal:

1. **Positive-Test:** Checks for the specified and delivered results and reactions.
2. **Negative-Test:** Checks expected false inputs and exceptional behavior. (It can be challenging to create certain effects, like network overload, to test this behavior).
3. **Robustness-Test:** Checks unexpected test cases that are not exception handled (catastrophical).

### Abstract vs. Concrete Test Cases

- **Abstract (Logical) Test Cases:** Test cases defined without concrete values for input data and expected results. They are helpful to document the scope and can be used with different concrete data.
- **Concrete Test Cases:** Use specific, defined input values and corresponding expected results.

Test cases in general should only consist of a **small number of steps**.

### Test Oracles

After executing a test case, it must be decided whether a failure was found by comparing the **actual result** to the **expected result** (desired result).

- **Requirement:** The expected result **must be specified for every test case beforehand**.
- **Test Oracles:** These are the sources that must be consulted to gather this information and deliver the desired results. Expected results are derived from the specification.

**Possibilities for Deriving Expected Values:**

1. **Tester Derivation:** The tester derives the expected value from the input value based on the specification (most common practice).
2. **Executable Prototype:** If a formal specification is available, an executable prototype can be generated, and its results can be used as the expected results for the real Software Under Test (SUT).
3. **Back-to-back Testing:** The SUT is independently implemented by several developer groups and all versions are tested against each other with the same input values. If results differ, at least one version has a failure.
4. **Other Oracles:** User manuals, (tested) predecessor versions, or programs with similar functions.
5. **System Itself:** Using the system itself (e.g., creating user profiles) is considered the **worst case**.
6. **Tolerance/Plausibility:** When expected values cannot always be predicted or computed, a **tolerance range** must be defined, or plausibility must be checked. Experience is always important.



## Psychology of Testing

Testing is defined as an extremely **creative and intellectually challenging task**. A core psychological challenge is that developers are human ("Errare humanum est – but who does like to admit?"). Development is constructive, whereas testing can be perceived as destructive.

### Developer Testing

|Pros|Cons|
|:--|:--|
|No introduction into the subject needed.|**Developer-Blindness:** If they implemented a fault (e.g., by misunderstanding the task), they will not find the fault through testing.|
|The developer knows the SUT very well.|They will not think of the necessary test case.|

### Independent Test Team

|Pros|Cons|
|:--|:--|
|**Unbiased**—it is not their "own" product.|Needs introduction; the tester must gain knowledge about the system to create test cases.|
|Wrong assumptions or misunderstandings of the developer do not affect the test team.||
|Possesses specific Testing Know-how.||

### Optimal Allocation

- Tests can be performed by the developer, colleagues within the same project, persons from other departments, or persons from other companies.
- Testing can/should always be performed **tool supported**.
- Allocation depends on the product and project.
- **It is important to combine independent testers and developers to test properly**.

### Reporting Failures

Failures must be reported to developers and management. Reporting must use a **neutral, objective, and constructive reporting style**. Undisturbed and open communication is essential.

**Reproducibility is important**. This requires documenting the testing environment and differences relative to the development environment. Mutual appreciation is also necessary.


## I. Software Development Models and Testing

A software development lifecycle model describes the types of activities for each phase in a software project and how they are related chronologically and logically. Models fall into two main categories: Sequential development models and Iterative-Incremental development models.

### A. V-Model (Sequential Development Model)

![[Pasted image 20251204035604.png]]

The V-Model connects development phases on the left branch (Construction Phases) with testing phases on the right branch (Test Phases).

|Construction Phase (Left Branch)|Associated Test Phase (Right Branch)|Purpose|
|:--|:--|:--|
|**Requirement Definition** (Wishes and features defined)|**Acceptance Testing** (Check if system fulfills requested features)|Validation|
|**Functional System Design** (Requirements mapped to functions)|**System Testing** (Check if complete system fulfills specified requirements)|Validation|
|**Technical System Design** (Interface definitions, system split into subsystems)|**Integration Testing** (Check if groups of units interact as specified)|Validation|
|**Component Specification** (Define task, behavior, interfaces for every component)|**Component Testing** (Check if units fulfill their specification)|Validation|
|**Implementation** (Implementation of every component)|**Component Testing**|Verification|

**Key Points:**

- The steps on the right branch represent **test execution**.
- Associated test preparation (planning, specification) starts earlier and runs **parallel** to the development steps.
- The rule of thumb is that **finding failures on the same level of abstraction on which they are created is easiest**.

### B. Iterative-Incremental Development Models (IIDMs)

![[Pasted image 20251204035633.png]]

Development takes place in small steps; the system is created through a planned series of versions and intermediate deliveries (increments).

- **Incremental:** Goals are specified from the beginning, and finished parts are assumed not to be changed anymore.
- **Iterative:** The system emerges over time; each iteration (cycle) typically specifies, designs, implements, and tests a group of features, delivering a **working software**. Iterations can include changes to finished features.
- **Testing in IIDMs:** Construction phases and test phases **interleave**. **Continuous integration tests** and **regression tests** are necessary. Verification and validation are possible for each increment.

---

## II. Core Testing Phases

### A. Unit Testing (Component Testing)

- **Phase:** The first testing phase.
- **Test Object:** Implemented software units (modules, classes) tested **in isolation** to exclude external influences, allowing failures to be clearly traced.
- **Goals:** Check if the component fulfills its **specified functionality** (input/output behavior). Other criteria include **Robustness** (capability to catch failure situations and keep working), **Efficiency** (economic resource usage, reaction times), and **Maintainability** (code structure, documentation).
- **Strategy and Responsibility:** Often performed by **developers** (developer test). Since source code is known, testing is often done as **white-box test**.
- **Environment:** Requires a **Driver** (calls services of the test object) and **Placeholder (Dummy/Stub)** (simulates services imported by the test object).

### B. Integration Testing

- **Phase:** The second testing phase, performed after unit testing. Components must be tested and failures fixed beforehand.
- **Goal:** To test if **communication between components works**. The primary goal is to find faults in **interfaces and interaction** between integrated components.
- **Fault Types:** Interface incompatibility, missing data, contradictory interpretation of specification, and timing problems (data communicated too late or at the wrong time).
- **Test Basis:** Software design, System design, Sequence Diagrams, and specification of interfaces and communication protocols.
- **Test Environment:** Needs a **Driver** and **Monitors** to observe the interfaces and data traffic.

|Integration Strategy|Description|Pros|Cons|
|:--|:--|:--|:--|
|**Top-Down**|Starts with high-level component (caller); lower levels replaced by dummies.|No or only simple drivers required.|Lower levels must be replaced by expensive dummies.|
|**Bottom-Up**|Starts with elemental components (non-callers); larger sub-systems created stepwise.|No dummies are necessary.|Requires drivers for higher-ordered components.|
|**Ad-Hoc**|Components randomly connected when finished.|Can be very fast (no "waiting times").|Requires both dummies and drivers.|
|**Big-Bang**|Everything is integrated once all components are finished.|-|All failures occur at once; localization and fixing faults is very hard and time-consuming.|

### C. System Testing

- **Phase:** The third test phase, performed after everything is integrated.
- **Test Object:** The **completely integrated system**.
- **Goal:** Test the system as a whole from the **customer’s point of view** to determine if it meets the specified system requirements. This involves **Verification** (system behavior meets design) and **Validation** (system is complete and works as expected).
- **Test Basis:** System and Software requirements specification, Use cases, and User stories.
- **Test Environment:** Should be very **close to the real production environment**, using real hardware and software products (instead of drivers and dummies).
- **Problems:** Vague customer specifications, or missing decisions regarding specified requirements.

### D. Acceptance Testing

- **Phase:** The final test phase; it focuses on the view and opinion of the customer.
- **Focus:** The focus is on **gaining trust** in the product, not failure finding. It is typically performed at the **customer’s location**.

|Acceptance Testing Type|Description|
|:--|:--|
|**Contract Acceptance Testing**|Performed against acceptance criteria recorded in the contract.|
|**Regulation Acceptance Testing**|Performed against regulations and standards the system must adhere to.|
|**Operational Acceptance Testing (OAT)**|Mostly performed by system administrators, testing backup/restore, disaster recovery, user management, and maintenance tasks.|
|**User Acceptance Testing (UAT)**|Recommended when the customer and the user of the system are different persons.|
|**Alpha Test**|Performed by selected customers at the **developer’s site**.|
|**Beta Test**|Performed by selected customers at the **customer’s site**.|

---

## III. Agile Testing in Scrum

The Agile Manifesto values working software, customer collaboration, and responding to change.

### A. Testing in Scrum

- Testing is **embedded into the single sprints**.
- The team should choose suitable techniques, and test automation (CI/CD tool chains) should be introduced.
- Manual testing should be viewed as **exploratory testing** (rapid cycle through test planning, without fixed test specifications).

### B. Agile Techniques

Test automation is very important in all testing phases.

- **Continuous Integration (CI):** Integrate and test changes frequently (after no more than a couple of hours).
- **Test First Programming (Test-Driven-Development):** Writing a failing automated test before changing any code, then improving the code until all tests pass.
- **Pair Programming:** Writing all production code with two people at one machine.
- **Incremental Design:** Investing in the design of the system every day.

### C. Agile Planning Definitions

- **Definition of Ready (DoR):** A checklist used to create user stories and quality assurance. A story is ready only if test cases can be formulated and the expected results are clear.
- **Definition of Done (DoD):** A checklist defining the goals for achieving a story. It specifies the testing goals, necessary testing techniques, test coverage, and test end criteria, ensuring product quality and customer satisfaction.
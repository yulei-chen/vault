<!DOCTYPE html> <html lang="en"><head>
<title>03. Neural Word Representation</title>
<base href="..">
<meta name="pathname" content="natural-language-processing/03.-neural-word-representation.html">
<meta name="description" content="Lecture Notes - 03. Neural Word Representation">
<meta property="og:title" content="03. Neural Word Representation">
<meta property="og:description" content="Lecture Notes - 03. Neural Word Representation">
<meta property="og:type" content="website">
<meta property="og:url" content="natural-language-processing/03.-neural-word-representation.html">
<meta property="og:image" content="undefined">
<meta charset="UTF-8"><meta property="og:site_name" content="Lecture Notes"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0"><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="site-lib/rss.xml"><script async="" id="webpage-script" src="site-lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-wasm-script" src="site-lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="site-lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="site-lib/media/favicon.png"><link rel="stylesheet" href="site-lib/styles/obsidian.css"><link rel="preload" href="site-lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="site-lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="site-lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="site-lib/styles/main-styles.css"></noscript><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-vertical-spacing:1.3em;--sidebar-margin:12px}:root{background-color:#202124}.sidebar{height:100%;font-size:14px;z-index:10;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));position:relative;overflow:hidden;overflow:clip;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}#left-sidebar{left:0}#right-sidebar{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}.sidebar.floating{position:absolute}.sidebar .leaf-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .leaf-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}#left-sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}#right-sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar #left-sidebar-content,.sidebar #right-sidebar-content{contain:none!important;container-type:normal!important;animation:none!important}.sidebar:has(.leaf-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:calc(2.3em + 2 * var(--sidebar-margin));width:var(--sidebar-width);padding:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}#left-sidebar .sidebar-topbar{left:0;flex-direction:row;border-top-right-radius:var(--radius-l)}#right-sidebar .sidebar-topbar{right:0;flex-direction:row-reverse;border-top-left-radius:var(--radius-l)}#left-sidebar .topbar-content{margin-right:calc(2.3em + var(--sidebar-margin));flex-direction:row}#right-sidebar .topbar-content{margin-left:calc(2.3em + var(--sidebar-margin));flex-direction:row-reverse}.topbar-content{overflow:hidden visible;overflow:clip visible;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:2px!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}#left-sidebar .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}#right-sidebar .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.feature-title{margin-left:1px;text-transform:uppercase;letter-spacing:.06em;margin-top:.75em;margin-bottom:.75em}.feature-header{display:flex;align-items:center;padding-top:0;font-size:1em;padding-left:0}body.floating-sidebars .sidebar{position:absolute}body{transition:background-color var(--color-fade-speed) ease-in-out}#navbar:not(:empty){display:flex;align-items:center;justify-content:space-between;padding:.5em 1em;width:100%}#main{display:flex;flex-direction:column;height:100%;width:100%;align-items:stretch;justify-content:center}#main-horizontal{display:flex;flex-direction:row;flex-grow:1;width:100%;align-items:stretch;justify-content:center}#center-content{flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0!important;transition:opacity .2s ease-in-out;pointer-events:none}#center-content>.obsidian-document{padding-left:2em;padding-right:1em;margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}body #center-content>.obsidian-document>.markdown-preview-sizer{padding-bottom:80vh;width:100%;max-width:var(--line-width);flex-basis:var(--line-width);transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}#center-content>.obsidian-document>div{width:100%!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}#center-content>.obsidian-document:not([data-type=markdown]).embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}#center-content>.obsidian-document:not([data-type=markdown]).embed>*{max-width:100%;max-height:100%;object-fit:contain}:not(h1,h2,h3,h4,h5,h6,li):has(> :is(.math,table)){overflow-x:auto!important}#center-content>.obsidian-document:not([data-type=markdown]){overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.obsidian-document[data-type=attachment]{display:flex;flex-direction:column;align-items:center;justify-content:center;height:100%;width:100%}.obsidian-document[data-type=attachment]>*{outline:0;border:none;box-shadow:none}.obsidian-document[data-type=attachment] :is(img){max-width:90%;max-height:90%;object-fit:contain}.obsidian-document[data-type=attachment]>:is(audio){width:100%;max-width:min(90%,var(--line-width))}.obsidian-document[data-type=attachment]>:is(embed,iframe,video){width:100%;height:100%;max-width:100%;max-height:100%;object-fit:contain}.canvas-wrapper>:is(.header,.footer){z-index:100;position:absolute;display:flex;justify-content:center;flex-direction:column;width:100%;align-items:center}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){let e=document.querySelectorAll("link[itemprop='include']");for(const t of e){let e=t.getAttribute("href");try{let o="";if(e.startsWith("https:")||e.startsWith("http:")||"file:"!=window.location.protocol){const n=await fetch(e);if(!n.ok){console.log("Could not include file: "+e),t?.remove();continue}o=await n.text()}else{const t=document.getElementById(btoa(encodeURI(e)));if(t){const e=JSON.parse(decodeURI(atob(t.getAttribute("value")??"")));o=e?.data??""}}let n=document.createRange().createContextualFragment(o);t.before(n),t.remove(),console.log("Included text: "+o),console.log("Included file: "+e)}catch(o){t?.remove(),console.log("Could not include file: "+e,o);continue}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script")));!function e(n){let l=o[n],c=n+1;l?(l&&"true"!=l.getAttribute("loaded")||n<o.length&&e(c),n<o.length&&l.addEventListener("load",(()=>e(c)))):n<o.length?e(c):t()}(0)}</script></head><body class="publish css-settings-manager show-inline-title show-ribbon is-focused"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="parsed-feature-container" style="display: contents;"><link itemprop="include" href="site-lib/html/custom-head-content-content.html"></div><div id="main"><div id="navbar"></div><div id="main-horizontal"><div id="left-content" class="leaf" style="--sidebar-width: var(--sidebar-width-left);"><div id="left-sidebar" class="sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><div id="search-container"><div id="search-wrapper"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div aria-label="Clear search" id="search-clear-button"></div></div></div></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content-wrapper"><div id="left-sidebar-content" class="leaf-content"><link itemprop="include" href="site-lib/html/file-tree-content.html"></div></div><script defer="">let ls = document.querySelector("#left-sidebar"); ls.classList.toggle("is-collapsed", window.innerWidth < 768); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div></div><div id="center-content" class="leaf"><div class="obsidian-document markdown-preview-view markdown-rendered node-insert-event is-readable-line-width allow-fold-headings allow-fold-lists show-indentation-guide show-properties" data-type="markdown"><style id="MJX-CHTML-styles">mjx-mfrac{display:inline-block;text-align:left}mjx-frac{display:inline-block;vertical-align:.17em;padding:0 .22em}mjx-frac[type="d"]{vertical-align:.04em}mjx-frac[delims]{padding:0 .1em}mjx-frac[atop]{padding:0 .12em}mjx-frac[atop][delims]{padding:0}mjx-dtable{display:inline-table;width:100%}mjx-dtable>*{font-size:2000%}mjx-dbox{display:block;font-size:5%}mjx-num{display:block;text-align:center}mjx-den{display:block;text-align:center}mjx-mfrac[bevelled]>mjx-num{display:inline-block}mjx-mfrac[bevelled]>mjx-den{display:inline-block}mjx-den[align=right],mjx-num[align=right]{text-align:right}mjx-den[align=left],mjx-num[align=left]{text-align:left}mjx-nstrut{display:inline-block;height:.054em;width:0;vertical-align:-.054em}mjx-nstrut[type="d"]{height:.217em;vertical-align:-.217em}mjx-dstrut{display:inline-block;height:.505em;width:0}mjx-dstrut[type="d"]{height:.726em}mjx-line{display:block;box-sizing:border-box;min-height:1px;height:.06em;border-top:.06em solid;margin:.06em -.1em;overflow:hidden}mjx-line[type="d"]{margin:.18em -.1em}mjx-mn{display:inline-block;text-align:left}mjx-c.mjx-c1D70E.TEX-I::before{padding:.431em .571em .011em 0;content:"σ"}mjx-c.mjx-c33::before{padding:.665em .5em .022em 0;content:"3"}mjx-c.mjx-c34::before{padding:.677em .5em 0 0;content:"4"}mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}mjx-assistive-mml{top:0;left:0;clip:rect(1px,1px,1px,1px);user-select:none;position:absolute!important;padding:1px 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;font-size-adjust:none;letter-spacing:normal;border-collapse:collapse;overflow-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-msup{display:inline-block;text-align:left}mjx-mi{display:inline-block;text-align:left}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url("site-lib/fonts/mathjax_zero.woff") format("woff")}@font-face{font-family:MJXTEX;src:url("site-lib/fonts/mathjax_main-regular.woff") format("woff")}@font-face{font-family:MJXTEX-B;src:url("site-lib/fonts/mathjax_main-bold.woff") format("woff")}@font-face{font-family:MJXTEX-I;src:url("site-lib/fonts/mathjax_math-italic.woff") format("woff")}@font-face{font-family:MJXTEX-MI;src:url("site-lib/fonts/mathjax_main-italic.woff") format("woff")}@font-face{font-family:MJXTEX-BI;src:url("site-lib/fonts/mathjax_math-bolditalic.woff") format("woff")}@font-face{font-family:MJXTEX-S1;src:url("site-lib/fonts/mathjax_size1-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S2;src:url("site-lib/fonts/mathjax_size2-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S3;src:url("site-lib/fonts/mathjax_size3-regular.woff") format("woff")}@font-face{font-family:MJXTEX-S4;src:url("site-lib/fonts/mathjax_size4-regular.woff") format("woff")}@font-face{font-family:MJXTEX-A;src:url("site-lib/fonts/mathjax_ams-regular.woff") format("woff")}@font-face{font-family:MJXTEX-C;src:url("site-lib/fonts/mathjax_calligraphic-regular.woff") format("woff")}@font-face{font-family:MJXTEX-CB;src:url("site-lib/fonts/mathjax_calligraphic-bold.woff") format("woff")}@font-face{font-family:MJXTEX-FR;src:url("site-lib/fonts/mathjax_fraktur-regular.woff") format("woff")}@font-face{font-family:MJXTEX-FRB;src:url("site-lib/fonts/mathjax_fraktur-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SS;src:url("site-lib/fonts/mathjax_sansserif-regular.woff") format("woff")}@font-face{font-family:MJXTEX-SSB;src:url("site-lib/fonts/mathjax_sansserif-bold.woff") format("woff")}@font-face{font-family:MJXTEX-SSI;src:url("site-lib/fonts/mathjax_sansserif-italic.woff") format("woff")}@font-face{font-family:MJXTEX-SC;src:url("site-lib/fonts/mathjax_script-regular.woff") format("woff")}@font-face{font-family:MJXTEX-T;src:url("site-lib/fonts/mathjax_typewriter-regular.woff") format("woff")}@font-face{font-family:MJXTEX-V;src:url("site-lib/fonts/mathjax_vector-regular.woff") format("woff")}@font-face{font-family:MJXTEX-VB;src:url("site-lib/fonts/mathjax_vector-bold.woff") format("woff")}mjx-c.mjx-c1D445.TEX-I::before{padding:.683em .759em .021em 0;content:"R"}mjx-c.mjx-c1D45A.TEX-I::before{padding:.442em .878em .011em 0;content:"m"}</style><div class="markdown-preview-sizer markdown-preview-section"><div class="header"><h1 class="page-title heading inline-title" id="3._Neural_Word_Representation_0">3. Neural Word Representation</h1><div class="data-bar"></div></div><div class="markdown-preview-pusher" style="width: 1px; height: 0.1px; margin-bottom: 0px;"></div><div class="el-p"><p dir="auto">NLP has undergone a <strong>major transformation</strong> over the past few decades, shifting from <strong>traditional statistical methods</strong> to <strong>modern neural network-based approaches</strong>.</p></div><div class="el-p"><p dir="auto">Early NLP systems relied heavily on <strong>statistical language models (SLMs)</strong>. These models estimate the probability of a word sequence based on observed frequencies in large text corpora. <strong>N-gram models</strong> is one of the most common approaches, which predict the next word given the previous n−1 words. For example, a <strong>trigram model</strong> predicts the next word based on the previous two words.</p></div><div class="el-p"><p dir="auto">However, SLMs have limitations about <strong>data sparsity</strong>, <strong>context window</strong>, and <strong>feature engineering</strong> (SLMs often required manual preprocessing and feature extraction).</p></div><div class="el-p"><p dir="auto">In the part, we will focus on <strong>neural network based methods</strong>.</p></div><div class="el-h2"><h2 data-heading="Word Embeddings" dir="auto" class="heading" id="Word_Embeddings_0"><span class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></span>Word Embeddings</h2></div><div class="el-p"><p dir="auto">Word Embedding is one of <strong>Dense Embeddings</strong>. Dense vector has the following advantages:</p></div><div class="el-ul"><ul class="has-list-bullet">
<li data-line="0" dir="auto"><span class="list-bullet"></span>
<p><strong>Short vectors</strong> may be easier to use as features in machine learning (less weights to tune)</p>
</li>
<li data-line="1" dir="auto"><span class="list-bullet"></span>
<p><strong>Dense vectors</strong> may generalize better than storing explicit counts</p>
</li>
<li data-line="2" dir="auto"><span class="list-bullet"></span><span class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></span>
<p>They may do better at capturing <mark>synonymy</mark>:</p>
<ul class="has-list-bullet">
<li data-line="4" dir="auto"><span class="list-bullet"></span>car and automobile are synonyms; but are distinct dimensions</li>
<li data-line="5" dir="auto"><span class="list-bullet"></span>a word with car as a neighbor and a word with automobile as a neighbor should be similar, but aren’t</li>
</ul>
</li>
<li data-line="6" dir="auto"><span class="list-bullet"></span>
<p>In practice, they appear to <strong>work better</strong></p>
</li>
</ul></div><div class="el-p"><p dir="auto">So can we convert <strong>one-hot (Sparse)</strong> to <strong>Dense Embeddings</strong>? Assume there is a matrix <strong>W</strong>. We can <mark>multiply our one-hot with <strong>W</strong></mark>. This gives us the “embedding” for a word. <strong>W is what we need to learn</strong> (in the context of machine learning).</p></div><div class="el-h2"><h2 data-heading="Language Models" dir="auto" class="heading" id="Language_Models_0"><span class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></span>Language Models</h2></div><div class="el-blockquote"><blockquote dir="auto">
<p>Please read Q&amp;A for more details.</p>
</blockquote></div><div class="el-ul"><ul class="has-list-bullet">
<li data-line="0" dir="auto"><span class="list-bullet"></span><span class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></span>
<p><strong>Word2Vec</strong></p>
<ul class="has-list-bullet">
<li data-line="2" dir="auto"><span class="list-bullet"></span>
<p><strong>CBOW</strong></p>
</li>
<li data-line="3" dir="auto"><span class="list-bullet"></span><span class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></span>
<p><strong>Skip-gram</strong></p>
<ul class="has-list-bullet">
<li data-line="5" dir="auto"><span class="list-bullet"></span><strong>Negative sampling</strong></li>
</ul>
</li>
</ul>
</li>
<li data-line="6" dir="auto"><span class="list-bullet"></span><span class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></span>
<p><strong>Fasttext</strong></p>
<ul class="has-list-bullet">
<li data-line="8" dir="auto"><span class="list-bullet"></span>character representation</li>
</ul>
</li>
<li data-line="9" dir="auto"><span class="list-bullet"></span>
<p><strong>Glove</strong></p>
</li>
</ul></div><div class="el-h2"><h2 data-heading="Q&amp;A" dir="auto" class="heading" id="Q&amp;A_0"><span class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></span>Q&amp;A</h2></div><div class="el-p"><p dir="auto"><strong>Q: What is the relationship between the corpus-based approach and feature engineering?</strong> A: In traditional Corpus-based Approaches to NLP, feature extraction is an explicit process. This feature extraction defines the features used by the statistical model, such as the word ID, case, and POS tags (e.g., Word: 546; Case: Upper-case; POS: NN). In Deep Learning Approaches, the input text still undergoes feature extraction, but the output is a dense binary vector (e.g., 10010010010101). Therefore, feature engineering is the traditional method of explicit feature extraction from the corpus, which has been largely replaced by <strong>automatic feature learning</strong> in deep learning.</p></div><div class="el-p"><p dir="auto"><strong>Q: What is an important property of word embeddings?</strong> A: An important property of word embeddings is that <strong>semantic similar words should have similar representation</strong>. This concept is tied to J.R. Firth’s 1953 dictum: “You shall know a word by the company it keeps!”.</p></div><div class="el-p"><p dir="auto"><strong>Q: What determines the size of one-hot embeddings?</strong> A: The size, or dimension, of a one-hot encoding vector is determined by the <strong>vocabulary size</strong>.</p></div><div class="el-p"><p dir="auto"><strong>Q: What is an advantage of dense embeddings?</strong> A: Dense vectors have several advantages: they are <strong>short vectors</strong>, may <strong>generalize better</strong>, and appear to <strong>work better in practice</strong>, especially at <strong>capturing synonymy</strong>.</p></div><div class="el-p"><p dir="auto"><strong>Q: What is a problem with n-gram models?</strong> A: A major problem with statistical N-gram models is that most longer word sequences have <strong>not been seen in the corpus</strong>. This requires a non-satisfactory solution like back-off to a shorter sequence. N-gram models also operate in <strong>Discrete Space</strong>.</p></div><div class="el-p"><p dir="auto"><strong>Q: Why do we call the Feed Forward Neural Network here self-supervised?</strong> A: The Feed Forward Neural Network language model is considered <strong>self-supervised</strong> because training needs only unlabelled data (pure text). The model is trained to predict the next word given previous words. The <strong>meaning of the word</strong> is important for estimating the next word, and this meaning needs to be encoded in the embedding.</p></div><div class="el-p"><p dir="auto"><strong>Q: What is the advantage of a continuous space?</strong> A: The advantage of mapping words to a <strong>continuous word representation</strong> is generalization to similar n-grams. Unlike discrete space, which only allows for exact matching, continuous space enables the model to automatically learn optimal features and find the nearest neighbor, capturing <strong>syntactic similarities</strong> and <strong>semantic similarities</strong>.</p></div><div class="el-p"><p dir="auto"><strong>Q: Is the position of the input words important?</strong> A: For the Continuous Bag of Words Model (CBOW), the position of the input words is <strong>ignored</strong>. The model sums the surrounding words to predict the center word.</p></div><div class="el-p"><p dir="auto"><strong>Q: How many embeddings do we get for each word?</strong> A: For Word2Vec models (CBOW and Skip-gram), typically there is <strong>one embedding for each word</strong>.</p></div><div class="el-blockquote"><blockquote dir="auto">
<p>More details: </p>
<ul class="has-list-bullet">
<li data-line="1" dir="auto"><span class="list-bullet"></span><a data-tooltip-position="top" aria-label="https://lilianweng.github.io/posts/2017-10-15-word-embedding/" rel="noopener nofollow" class="external-link is-unresolved" href="https://lilianweng.github.io/posts/2017-10-15-word-embedding/" target="_self">Learning Word Embedding</a> </li>
<li data-line="2" dir="auto"><span class="list-bullet"></span><a data-tooltip-position="top" aria-label="https://medium.com/@fraidoonomarzai99/word2vec-cbow-skip-gram-in-depth-88d9cc340a50" rel="noopener nofollow" class="external-link is-unresolved" href="https://medium.com/@fraidoonomarzai99/word2vec-cbow-skip-gram-in-depth-88d9cc340a50" target="_self">Word2Vec (CBOW, Skip-gram) In Depth</a></li>
</ul>
</blockquote></div><div class="el-p"><p dir="auto"><strong>Q: What is the CBOW score function based on?</strong> A: The CBOW score function is based on the <strong>dot product</strong> to measure similarity.</p></div><div class="el-p"><p dir="auto"><strong>Q: What is the difference between CBOW and Skip-gram?</strong> A: CBOW and Skip-gram have opposite objectives:</p></div><div class="el-ul"><ul class="has-list-bullet">
<li data-line="0" dir="auto"><span class="list-bullet"></span><strong>CBOW predicts the word based on surrounding words</strong>.</li>
<li data-line="1" dir="auto"><span class="list-bullet"></span><strong>Skip-gram predicts the surrounding words given the current word</strong>.</li>
</ul></div><div class="el-p"><p dir="auto"><strong>Q: What is maximized with the Skip-gram objective?</strong> A: The Skip-gram objective is to <strong>maximize the probability</strong> of any context word given the current word. This objective uses a <strong>negative log loss</strong>.</p></div><div class="el-p"><p dir="auto"><strong>Q: Why is negative sampling necessary?</strong> A: Negative sampling is necessary because the standard Skip-gram objective requires <strong>normalization over the entire vocabulary</strong>, which is computationally demanding. Negative sampling resolves this by generating samples and summing over those samples instead of the entire vocabulary.<br>
<iframe class="external-embed mod-receives-events is-unresolved" sandbox="allow-forms allow-presentation allow-same-origin allow-popups-to-escape-sandbox allow-scripts allow-modals allow-popups" allow="fullscreen" frameborder="0" referrerpolicy="strict-origin-when-cross-origin" src="https://releases.obsidian.md/youtube?v=viZrOnJclY0" loading="lazy" target="_self"></iframe></p></div><div class="el-p"><p dir="auto"><strong>Q: Why do we use sigmoid instead of softmax in the objective function?</strong> A: We use the <strong>Sigmoid function</strong> (<span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70E TEX-I"></mjx-c></mjx-mi></mjx-math></mjx-container></span>) because negative sampling converts the problem from a multiclass classifier to a <strong>binary classifier</strong>.</p></div><div class="el-p"><p dir="auto"><strong>Q: Where do we get negative examples from?</strong> A: Negative examples are sampled from the <strong>lexicon</strong>. The negative set typically contains 2 to 20 words sampled from some distribution, such as uniform, unigram, or a <strong>smoothed unigram</strong>. The smoothed unigram distribution is created by raising the probabilities to the power of <span class="math math-inline is-loaded"><mjx-container class="MathJax" jax="CHTML"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container></span> and then renormalizing.</p></div><div class="el-p"><p dir="auto"><strong>Q: For a corpus with fixed size, are there more unique 6-grams or 3-grams?</strong> A: Based on the problem of N-gram models—that most longer word sequences have not been seen—it can be inferred that there would be <strong>more unique 6-grams</strong> than 3-grams, as longer sequences lead to higher sparsity.</p></div><div class="el-p"><p dir="auto"><strong>Q: How are context and target words represented with FastText?</strong> A: FastText uses <strong>character N-grams</strong>:</p></div><div class="el-p"><p dir="auto">A: FastText uses character N-grams:</p></div><div class="el-ul"><ul class="has-list-bullet">
<li data-line="0" dir="auto"><span class="list-bullet"></span>The word embedding for the target word is the <strong>sum of character 3-grams to 6-grams and the whole word itself</strong>.</li>
<li data-line="1" dir="auto"><span class="list-bullet"></span>The word embedding for the context word is taken from the embedding table <strong>without adding character n-grams</strong>.</li>
</ul></div><div class="el-p"><p dir="auto"><strong>Q: What is the difference between Word2Vec and FastText?</strong> A: FastText is an extension of Word2Vec that addresses challenges found in Word2Vec. FastText uses <strong>subword representation (character n-grams)</strong> to solve issues in morphologically rich languages and to create representations for words that have not been seen previously.</p></div><div class="el-p"><p dir="auto"><strong>Q: What is a key difference between Word2Vec and GloVe?</strong> A: The key difference lies in the perspective used for training: Word2Vec uses a <strong>Local view</strong> (predicting words within the context), while GloVe uses a <strong>Global view</strong>. GloVe leverages <strong>Word co-occurrence matrices</strong> and is motivated by the <strong>ratio of co-occurrence probabilities</strong>.</p></div><div class="el-p"><p dir="auto"><strong>Q: In which situations does GloVe likely produce unreliable word embeddings?</strong></p></div><div class="el-ul"><ul class="has-list-bullet">
<li data-line="0" dir="auto"><span class="list-bullet"></span><strong>Rare words</strong> – too few co-occurrences to learn meaningful vectors.</li>
<li data-line="1" dir="auto"><span class="list-bullet"></span><strong>Small corpus</strong> – global statistics are sparse and noisy.</li>
<li data-line="2" dir="auto"><span class="list-bullet"></span><strong>Domain mismatch</strong> – training domain differs from the target domain.</li>
<li data-line="3" dir="auto"><span class="list-bullet"></span><strong>Polysemous words</strong> – one vector cannot represent multiple senses.</li>
<li data-line="4" dir="auto"><span class="list-bullet"></span><strong>No subword modeling</strong> – fails on morphology and misspellings.</li>
<li data-line="5" dir="auto"><span class="list-bullet"></span><strong>Out-of-vocabulary words</strong> – no embeddings for unseen words.</li>
<li data-line="6" dir="auto"><span class="list-bullet"></span><strong>Biased training data</strong> – co-occurrence statistics reflect and amplify bias.</li>
</ul></div><div class="footer"><div class="data-bar"></div></div></div></div></div><div id="right-content" class="leaf" style="--sidebar-width: var(--sidebar-width-right);"><div id="right-sidebar" class="sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme-toggle-input" id=""><input class="theme-toggle-input" type="checkbox" id="theme-toggle-input"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content-wrapper"><div id="right-sidebar-content" class="leaf-content"><div class="graph-view-wrapper"><div class="feature-header"><div class="feature-title">Interactive Graph</div></div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<div class="graph-icon graph-global" role="button" aria-label="Global Graph" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-git-fork"><circle cx="12" cy="18" r="3"></circle><circle cx="6" cy="6" r="3"></circle><circle cx="18" cy="6" r="3"></circle><path d="M18 9v2c0 .6-.4 1-1 1H7c-.6 0-1-.4-1-1V9"></path><path d="M12 12v3"></path></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div id="outline" class=" tree-container"><div class="feature-header"><div class="feature-title">Table Of Contents</div><button class="clickable-icon nav-action-button tree-collapse-all" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-item mod-collapsible" data-depth="1"><a class="tree-item-self is-clickable mod-collapsible" href="natural-language-processing/03.-neural-word-representation.html#3._Neural_Word_Representation_0" data-path="#3._Neural_Word_Representation_0"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><div class="tree-item-inner heading-link" heading-name="3. Neural Word Representation">3. Neural Word Representation</div></a><div class="tree-item-children"><div class="tree-item" data-depth="2"><a class="tree-item-self is-clickable" href="natural-language-processing/03.-neural-word-representation.html#Word_Embeddings_0" data-path="#Word_Embeddings_0"><div class="tree-item-inner heading-link" heading-name="Word Embeddings">Word Embeddings</div></a><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-item-self is-clickable" href="natural-language-processing/03.-neural-word-representation.html#Language_Models_0" data-path="#Language_Models_0"><div class="tree-item-inner heading-link" heading-name="Language Models">Language Models</div></a><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-item-self is-clickable" href="natural-language-processing/03.-neural-word-representation.html#Q&amp;A_0" data-path="#Q&amp;A_0"><div class="tree-item-inner heading-link" heading-name="Q&amp;A">Q&amp;A</div></a><div class="tree-item-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector("#right-sidebar"); rs.classList.toggle("is-collapsed", window.innerWidth < 768); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></div></div></body></html>